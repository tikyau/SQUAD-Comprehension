{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from collections import Counter\n",
    "from time import sleep\n",
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 2412444),\n",
       " ('LOCATION', 76720),\n",
       " ('ORGANIZATION', 61678),\n",
       " ('PERSON', 58985)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('data/ner_train.json') as json_data:\n",
    "    d = json.load(json_data)\n",
    "\n",
    "text=[]\n",
    "l=0\n",
    "\n",
    "for i in d:\n",
    "    for j in i:\n",
    "        for k in j:\n",
    "            text.append(k)\n",
    "\n",
    "tt = []\n",
    "for i in range(len(text)):\n",
    "    if(text[i]!=''):\n",
    "        tt.append(text[i])\n",
    "\n",
    "text = tt\n",
    "\n",
    "word_counts = Counter(text)\n",
    "\n",
    "word_counts.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: A tuple of dicts.  The first dict....\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def get_target(words, idx, window_size=5):\n",
    "    ''' Get a list of words in a window around an index. '''\n",
    "    \n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    target_words = set(words[start:idx] + words[idx+1:stop+1])\n",
    "    \n",
    "    return list(target_words)\n",
    "\n",
    "\n",
    "def get_batches(words, batch_size, window_size=5):\n",
    "    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n",
    "    \n",
    "    n_batches = len(words)//batch_size\n",
    "    \n",
    "    # only full batches\n",
    "    words = words[:n_batches*batch_size]\n",
    "    for idx in range(0, len(words), batch_size):\n",
    "        x, y = [], []\n",
    "        batch = words[idx:idx+batch_size]\n",
    "        for ii in range(len(batch)):\n",
    "            batch_x = batch[ii]\n",
    "            batch_y = get_target(batch, ii, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 2609827\n",
      "Unique words: 4\n"
     ]
    }
   ],
   "source": [
    "words = text\n",
    "\n",
    "print(\"Total words: {}\".format(len(words)))\n",
    "print(\"Unique words: {}\".format(len(set(words))))\n",
    "\n",
    "# vocab_to_int, int_to_vocab = utils.create_lookup_tables(words)\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(words)\n",
    "int_words = [vocab_to_int[word] for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_word():\n",
    "    threshold = random.uniform(0.6, 0.9)\n",
    "    word_counts = Counter(int_words)\n",
    "    total_count = len(int_words)\n",
    "    freqs = {word: count/total_count for word, count in word_counts.items()}\n",
    "    p_drop = {word: 1 - np.sqrt(threshold/freqs[word]) for word in word_counts}\n",
    "    train_words = [word for word in int_words if random.random() < (1 - p_drop[word])]\n",
    "    return train_words, threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "window_size = 2\n",
    "n_vocab = len(int_to_vocab)\n",
    "n_embedding =  25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/device:CPU:0'):\n",
    "\n",
    "    train_graph = tf.Graph()\n",
    "    with train_graph.as_default():\n",
    "        inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    #     labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "        \n",
    "        embedding = tf.Variable(tf.random_uniform((n_vocab, n_embedding), -1, 1))\n",
    "        embed = tf.nn.embedding_lookup(embedding, inputs) # use tf.nn.embedding_lookup to get the hidden layer output\n",
    "        \n",
    "        softmax_w = tf.Variable(tf.truncated_normal((n_vocab, n_embedding))) # create softmax weight matrix here\n",
    "        softmax_b = tf.Variable(tf.zeros(n_vocab), name=\"softmax_bias\") # create softmax biases here\n",
    "        \n",
    "        logits = tf.matmul(embed, tf.transpose(softmax_w)) + softmax_b\n",
    "        labels_one_hot = tf.one_hot(labels, n_vocab)\n",
    "\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels_one_hot, logits=logits)\n",
    "        cost = tf.reduce_mean(loss)\n",
    "        \n",
    "        global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer().minimize(cost, global_step=global_step)\n",
    "        \n",
    "         ## From Thushan Ganegedara's implementation\n",
    "        valid_size = 3 # Random set of words to evaluate similarity on.\n",
    "        valid_window = n_vocab\n",
    "        # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "        valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "#         valid_examples = np.append(valid_examples, \n",
    "#                                    random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "        valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "        # We use the cosine distance:\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "        normalized_embedding = embedding / norm\n",
    "        valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "        similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘checkpoints/ner’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "# If the checkpoints directory doesn't exist:\n",
    "!mkdir checkpoints/ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Threshold: 0.7253311070144872 Length of Training words: 2334060\n",
      "Global Step: 100 Epoch 1/50 Iteration: 100 Avg. Training loss: 1.0828 0.0071 sec/batch\n",
      "Global Step: 200 Epoch 1/50 Iteration: 200 Avg. Training loss: 0.5837 0.0073 sec/batch\n",
      "Global Step: 300 Epoch 1/50 Iteration: 300 Avg. Training loss: 0.5848 0.0072 sec/batch\n",
      "Global Step: 400 Epoch 1/50 Iteration: 400 Avg. Training loss: 0.5640 0.0072 sec/batch\n",
      "Global Step: 500 Epoch 1/50 Iteration: 500 Avg. Training loss: 0.5213 0.0071 sec/batch\n",
      "Global Step: 600 Epoch 1/50 Iteration: 600 Avg. Training loss: 0.5166 0.0069 sec/batch\n",
      "Global Step: 700 Epoch 1/50 Iteration: 700 Avg. Training loss: 0.4854 0.0070 sec/batch\n",
      "Global Step: 800 Epoch 1/50 Iteration: 800 Avg. Training loss: 0.5632 0.0069 sec/batch\n",
      "Global Step: 900 Epoch 1/50 Iteration: 900 Avg. Training loss: 0.5081 0.0067 sec/batch\n",
      "Global Step: 1000 Epoch 1/50 Iteration: 1000 Avg. Training loss: 0.5941 0.0072 sec/batch\n",
      "Global Step: 1100 Epoch 1/50 Iteration: 1100 Avg. Training loss: 0.6114 0.0090 sec/batch\n",
      "Global Step: 1200 Epoch 1/50 Iteration: 1200 Avg. Training loss: 0.5679 0.0084 sec/batch\n",
      "Global Step: 1300 Epoch 1/50 Iteration: 1300 Avg. Training loss: 0.5829 0.0097 sec/batch\n",
      "Global Step: 1400 Epoch 1/50 Iteration: 1400 Avg. Training loss: 0.4982 0.0085 sec/batch\n",
      "Global Step: 1500 Epoch 1/50 Iteration: 1500 Avg. Training loss: 0.5486 0.0075 sec/batch\n",
      "Global Step: 1600 Epoch 1/50 Iteration: 1600 Avg. Training loss: 0.5839 0.0068 sec/batch\n",
      "Global Step: 1700 Epoch 1/50 Iteration: 1700 Avg. Training loss: 0.4773 0.0069 sec/batch\n",
      "Global Step: 1800 Epoch 1/50 Iteration: 1800 Avg. Training loss: 0.5136 0.0063 sec/batch\n",
      "Global Step: 1900 Epoch 1/50 Iteration: 1900 Avg. Training loss: 0.5967 0.0069 sec/batch\n",
      "Global Step: 2000 Epoch 1/50 Iteration: 2000 Avg. Training loss: 0.4745 0.0077 sec/batch\n",
      "Global Step: 2100 Epoch 1/50 Iteration: 2100 Avg. Training loss: 0.5571 0.0083 sec/batch\n",
      "Global Step: 2200 Epoch 1/50 Iteration: 2200 Avg. Training loss: 0.4961 0.0079 sec/batch\n",
      "Global Step: 2300 Epoch 1/50 Iteration: 2300 Avg. Training loss: 0.5608 0.0067 sec/batch\n",
      "Epoch 2/50 Threshold: 0.6554824260004626 Length of Training words: 2229177\n",
      "Global Step: 2400 Epoch 2/50 Iteration: 2400 Avg. Training loss: 0.6063 0.0047 sec/batch\n",
      "Global Step: 2500 Epoch 2/50 Iteration: 2500 Avg. Training loss: 0.5584 0.0093 sec/batch\n",
      "Global Step: 2600 Epoch 2/50 Iteration: 2600 Avg. Training loss: 0.5631 0.0065 sec/batch\n",
      "Global Step: 2700 Epoch 2/50 Iteration: 2700 Avg. Training loss: 0.5686 0.0078 sec/batch\n",
      "Global Step: 2800 Epoch 2/50 Iteration: 2800 Avg. Training loss: 0.5389 0.0065 sec/batch\n",
      "Global Step: 2900 Epoch 2/50 Iteration: 2900 Avg. Training loss: 0.5343 0.0082 sec/batch\n",
      "Global Step: 3000 Epoch 2/50 Iteration: 3000 Avg. Training loss: 0.5125 0.0068 sec/batch\n",
      "Global Step: 3100 Epoch 2/50 Iteration: 3100 Avg. Training loss: 0.5685 0.0066 sec/batch\n",
      "Global Step: 3200 Epoch 2/50 Iteration: 3200 Avg. Training loss: 0.5273 0.0067 sec/batch\n",
      "Global Step: 3300 Epoch 2/50 Iteration: 3300 Avg. Training loss: 0.6179 0.0086 sec/batch\n",
      "Global Step: 3400 Epoch 2/50 Iteration: 3400 Avg. Training loss: 0.6179 0.0067 sec/batch\n",
      "Global Step: 3500 Epoch 2/50 Iteration: 3500 Avg. Training loss: 0.5765 0.0077 sec/batch\n",
      "Global Step: 3600 Epoch 2/50 Iteration: 3600 Avg. Training loss: 0.5838 0.0067 sec/batch\n",
      "Global Step: 3700 Epoch 2/50 Iteration: 3700 Avg. Training loss: 0.5241 0.0076 sec/batch\n",
      "Global Step: 3800 Epoch 2/50 Iteration: 3800 Avg. Training loss: 0.5565 0.0067 sec/batch\n",
      "Global Step: 3900 Epoch 2/50 Iteration: 3900 Avg. Training loss: 0.5891 0.0079 sec/batch\n",
      "Global Step: 4000 Epoch 2/50 Iteration: 4000 Avg. Training loss: 0.5275 0.0066 sec/batch\n",
      "Global Step: 4100 Epoch 2/50 Iteration: 4100 Avg. Training loss: 0.5309 0.0066 sec/batch\n",
      "Global Step: 4200 Epoch 2/50 Iteration: 4200 Avg. Training loss: 0.5613 0.0065 sec/batch\n",
      "Global Step: 4300 Epoch 2/50 Iteration: 4300 Avg. Training loss: 0.4863 0.0070 sec/batch\n",
      "Global Step: 4400 Epoch 2/50 Iteration: 4400 Avg. Training loss: 0.6082 0.0069 sec/batch\n",
      "Global Step: 4500 Epoch 2/50 Iteration: 4500 Avg. Training loss: 0.5056 0.0063 sec/batch\n",
      "Epoch 3/50 Threshold: 0.6969718819208395 Length of Training words: 2292005\n",
      "Global Step: 4600 Epoch 3/50 Iteration: 4600 Avg. Training loss: 0.6058 0.0026 sec/batch\n",
      "Global Step: 4700 Epoch 3/50 Iteration: 4700 Avg. Training loss: 0.6067 0.0069 sec/batch\n",
      "Global Step: 4800 Epoch 3/50 Iteration: 4800 Avg. Training loss: 0.4908 0.0064 sec/batch\n",
      "Global Step: 4900 Epoch 3/50 Iteration: 4900 Avg. Training loss: 0.5973 0.0067 sec/batch\n",
      "Global Step: 5000 Epoch 3/50 Iteration: 5000 Avg. Training loss: 0.5489 0.0066 sec/batch\n",
      "Global Step: 5100 Epoch 3/50 Iteration: 5100 Avg. Training loss: 0.4831 0.0089 sec/batch\n",
      "Global Step: 5200 Epoch 3/50 Iteration: 5200 Avg. Training loss: 0.5188 0.0064 sec/batch\n",
      "Global Step: 5300 Epoch 3/50 Iteration: 5300 Avg. Training loss: 0.5327 0.0066 sec/batch\n",
      "Global Step: 5400 Epoch 3/50 Iteration: 5400 Avg. Training loss: 0.5512 0.0079 sec/batch\n",
      "Global Step: 5500 Epoch 3/50 Iteration: 5500 Avg. Training loss: 0.6028 0.0076 sec/batch\n",
      "Global Step: 5600 Epoch 3/50 Iteration: 5600 Avg. Training loss: 0.5954 0.0082 sec/batch\n",
      "Global Step: 5700 Epoch 3/50 Iteration: 5700 Avg. Training loss: 0.5783 0.0068 sec/batch\n",
      "Global Step: 5800 Epoch 3/50 Iteration: 5800 Avg. Training loss: 0.5890 0.0075 sec/batch\n",
      "Global Step: 5900 Epoch 3/50 Iteration: 5900 Avg. Training loss: 0.5572 0.0097 sec/batch\n",
      "Global Step: 6000 Epoch 3/50 Iteration: 6000 Avg. Training loss: 0.5426 0.0066 sec/batch\n",
      "Global Step: 6100 Epoch 3/50 Iteration: 6100 Avg. Training loss: 0.5196 0.0067 sec/batch\n",
      "Global Step: 6200 Epoch 3/50 Iteration: 6200 Avg. Training loss: 0.5608 0.0094 sec/batch\n",
      "Global Step: 6300 Epoch 3/50 Iteration: 6300 Avg. Training loss: 0.5021 0.0070 sec/batch\n",
      "Global Step: 6400 Epoch 3/50 Iteration: 6400 Avg. Training loss: 0.5650 0.0082 sec/batch\n",
      "Global Step: 6500 Epoch 3/50 Iteration: 6500 Avg. Training loss: 0.5260 0.0069 sec/batch\n",
      "Global Step: 6600 Epoch 3/50 Iteration: 6600 Avg. Training loss: 0.5019 0.0067 sec/batch\n",
      "Global Step: 6700 Epoch 3/50 Iteration: 6700 Avg. Training loss: 0.5856 0.0067 sec/batch\n",
      "Global Step: 6800 Epoch 3/50 Iteration: 6800 Avg. Training loss: 0.5021 0.0063 sec/batch\n",
      "Epoch 4/50 Threshold: 0.8574214552080841 Length of Training words: 2520256\n",
      "Global Step: 6900 Epoch 4/50 Iteration: 6900 Avg. Training loss: 0.5887 0.0032 sec/batch\n",
      "Global Step: 7000 Epoch 4/50 Iteration: 7000 Avg. Training loss: 0.5845 0.0068 sec/batch\n",
      "Global Step: 7100 Epoch 4/50 Iteration: 7100 Avg. Training loss: 0.4799 0.0063 sec/batch\n",
      "Global Step: 7200 Epoch 4/50 Iteration: 7200 Avg. Training loss: 0.5693 0.0068 sec/batch\n",
      "Global Step: 7300 Epoch 4/50 Iteration: 7300 Avg. Training loss: 0.5112 0.0069 sec/batch\n",
      "Global Step: 7400 Epoch 4/50 Iteration: 7400 Avg. Training loss: 0.4915 0.0071 sec/batch\n",
      "Global Step: 7500 Epoch 4/50 Iteration: 7500 Avg. Training loss: 0.4823 0.0064 sec/batch\n",
      "Global Step: 7600 Epoch 4/50 Iteration: 7600 Avg. Training loss: 0.4793 0.0068 sec/batch\n",
      "Global Step: 7700 Epoch 4/50 Iteration: 7700 Avg. Training loss: 0.5004 0.0069 sec/batch\n",
      "Global Step: 7800 Epoch 4/50 Iteration: 7800 Avg. Training loss: 0.5159 0.0066 sec/batch\n",
      "Global Step: 7900 Epoch 4/50 Iteration: 7900 Avg. Training loss: 0.5745 0.0067 sec/batch\n",
      "Global Step: 8000 Epoch 4/50 Iteration: 8000 Avg. Training loss: 0.5449 0.0076 sec/batch\n",
      "Global Step: 8100 Epoch 4/50 Iteration: 8100 Avg. Training loss: 0.5575 0.0103 sec/batch\n",
      "Global Step: 8200 Epoch 4/50 Iteration: 8200 Avg. Training loss: 0.5706 0.0099 sec/batch\n",
      "Global Step: 8300 Epoch 4/50 Iteration: 8300 Avg. Training loss: 0.5014 0.0065 sec/batch\n",
      "Global Step: 8400 Epoch 4/50 Iteration: 8400 Avg. Training loss: 0.5005 0.0076 sec/batch\n",
      "Global Step: 8500 Epoch 4/50 Iteration: 8500 Avg. Training loss: 0.5446 0.0070 sec/batch\n",
      "Global Step: 8600 Epoch 4/50 Iteration: 8600 Avg. Training loss: 0.5494 0.0064 sec/batch\n",
      "Global Step: 8700 Epoch 4/50 Iteration: 8700 Avg. Training loss: 0.4309 0.0065 sec/batch\n",
      "Global Step: 8800 Epoch 4/50 Iteration: 8800 Avg. Training loss: 0.5083 0.0078 sec/batch\n",
      "Global Step: 8900 Epoch 4/50 Iteration: 8900 Avg. Training loss: 0.5781 0.0078 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 9000 Epoch 4/50 Iteration: 9000 Avg. Training loss: 0.4530 0.0079 sec/batch\n",
      "Global Step: 9100 Epoch 4/50 Iteration: 9100 Avg. Training loss: 0.4890 0.0080 sec/batch\n",
      "Global Step: 9200 Epoch 4/50 Iteration: 9200 Avg. Training loss: 0.5549 0.0065 sec/batch\n",
      "Global Step: 9300 Epoch 4/50 Iteration: 9300 Avg. Training loss: 0.4613 0.0074 sec/batch\n",
      "Epoch 5/50 Threshold: 0.704454348766629 Length of Training words: 2303960\n",
      "Global Step: 9400 Epoch 5/50 Iteration: 9400 Avg. Training loss: 0.5777 0.0030 sec/batch\n",
      "Global Step: 9500 Epoch 5/50 Iteration: 9500 Avg. Training loss: 0.6077 0.0066 sec/batch\n",
      "Global Step: 9600 Epoch 5/50 Iteration: 9600 Avg. Training loss: 0.5087 0.0074 sec/batch\n",
      "Global Step: 9700 Epoch 5/50 Iteration: 9700 Avg. Training loss: 0.5940 0.0073 sec/batch\n",
      "Global Step: 9800 Epoch 5/50 Iteration: 9800 Avg. Training loss: 0.5221 0.0076 sec/batch\n",
      "Global Step: 9900 Epoch 5/50 Iteration: 9900 Avg. Training loss: 0.5127 0.0063 sec/batch\n",
      "Global Step: 10000 Epoch 5/50 Iteration: 10000 Avg. Training loss: 0.5023 0.0069 sec/batch\n",
      "Global Step: 10100 Epoch 5/50 Iteration: 10100 Avg. Training loss: 0.4920 0.0082 sec/batch\n",
      "Global Step: 10200 Epoch 5/50 Iteration: 10200 Avg. Training loss: 0.5792 0.0073 sec/batch\n",
      "Global Step: 10300 Epoch 5/50 Iteration: 10300 Avg. Training loss: 0.5893 0.0087 sec/batch\n",
      "Global Step: 10400 Epoch 5/50 Iteration: 10400 Avg. Training loss: 0.6018 0.0068 sec/batch\n",
      "Global Step: 10500 Epoch 5/50 Iteration: 10500 Avg. Training loss: 0.5852 0.0067 sec/batch\n",
      "Global Step: 10600 Epoch 5/50 Iteration: 10600 Avg. Training loss: 0.5822 0.0066 sec/batch\n",
      "Global Step: 10700 Epoch 5/50 Iteration: 10700 Avg. Training loss: 0.5412 0.0075 sec/batch\n",
      "Global Step: 10800 Epoch 5/50 Iteration: 10800 Avg. Training loss: 0.5277 0.0066 sec/batch\n",
      "Global Step: 10900 Epoch 5/50 Iteration: 10900 Avg. Training loss: 0.5393 0.0076 sec/batch\n",
      "Global Step: 11000 Epoch 5/50 Iteration: 11000 Avg. Training loss: 0.5627 0.0081 sec/batch\n",
      "Global Step: 11100 Epoch 5/50 Iteration: 11100 Avg. Training loss: 0.5236 0.0102 sec/batch\n",
      "Global Step: 11200 Epoch 5/50 Iteration: 11200 Avg. Training loss: 0.5263 0.0097 sec/batch\n",
      "Global Step: 11300 Epoch 5/50 Iteration: 11300 Avg. Training loss: 0.5512 0.0066 sec/batch\n",
      "Global Step: 11400 Epoch 5/50 Iteration: 11400 Avg. Training loss: 0.4958 0.0066 sec/batch\n",
      "Global Step: 11500 Epoch 5/50 Iteration: 11500 Avg. Training loss: 0.5652 0.0067 sec/batch\n",
      "Global Step: 11600 Epoch 5/50 Iteration: 11600 Avg. Training loss: 0.5025 0.0068 sec/batch\n",
      "Epoch 6/50 Threshold: 0.6540720500319877 Length of Training words: 2226793\n",
      "Global Step: 11700 Epoch 6/50 Iteration: 11700 Avg. Training loss: 0.5981 0.0016 sec/batch\n",
      "Global Step: 11800 Epoch 6/50 Iteration: 11800 Avg. Training loss: 0.6187 0.0069 sec/batch\n",
      "Global Step: 11900 Epoch 6/50 Iteration: 11900 Avg. Training loss: 0.5125 0.0065 sec/batch\n",
      "Global Step: 12000 Epoch 6/50 Iteration: 12000 Avg. Training loss: 0.6024 0.0075 sec/batch\n",
      "Global Step: 12100 Epoch 6/50 Iteration: 12100 Avg. Training loss: 0.5445 0.0076 sec/batch\n",
      "Global Step: 12200 Epoch 6/50 Iteration: 12200 Avg. Training loss: 0.4992 0.0073 sec/batch\n",
      "Global Step: 12300 Epoch 6/50 Iteration: 12300 Avg. Training loss: 0.5291 0.0069 sec/batch\n",
      "Global Step: 12400 Epoch 6/50 Iteration: 12400 Avg. Training loss: 0.5598 0.0072 sec/batch\n",
      "Global Step: 12500 Epoch 6/50 Iteration: 12500 Avg. Training loss: 0.5375 0.0071 sec/batch\n",
      "Global Step: 12600 Epoch 6/50 Iteration: 12600 Avg. Training loss: 0.6259 0.0063 sec/batch\n",
      "Global Step: 12700 Epoch 6/50 Iteration: 12700 Avg. Training loss: 0.5680 0.0068 sec/batch\n",
      "Global Step: 12800 Epoch 6/50 Iteration: 12800 Avg. Training loss: 0.6322 0.0068 sec/batch\n",
      "Global Step: 12900 Epoch 6/50 Iteration: 12900 Avg. Training loss: 0.5904 0.0065 sec/batch\n",
      "Global Step: 13000 Epoch 6/50 Iteration: 13000 Avg. Training loss: 0.5212 0.0067 sec/batch\n",
      "Global Step: 13100 Epoch 6/50 Iteration: 13100 Avg. Training loss: 0.5529 0.0067 sec/batch\n",
      "Global Step: 13200 Epoch 6/50 Iteration: 13200 Avg. Training loss: 0.5853 0.0065 sec/batch\n",
      "Global Step: 13300 Epoch 6/50 Iteration: 13300 Avg. Training loss: 0.5120 0.0069 sec/batch\n",
      "Global Step: 13400 Epoch 6/50 Iteration: 13400 Avg. Training loss: 0.5244 0.0072 sec/batch\n",
      "Global Step: 13500 Epoch 6/50 Iteration: 13500 Avg. Training loss: 0.6026 0.0066 sec/batch\n",
      "Global Step: 13600 Epoch 6/50 Iteration: 13600 Avg. Training loss: 0.4862 0.0065 sec/batch\n",
      "Global Step: 13700 Epoch 6/50 Iteration: 13700 Avg. Training loss: 0.5958 0.0068 sec/batch\n",
      "Global Step: 13800 Epoch 6/50 Iteration: 13800 Avg. Training loss: 0.5129 0.0071 sec/batch\n",
      "Global Step: 13900 Epoch 6/50 Iteration: 13900 Avg. Training loss: 0.5613 0.0067 sec/batch\n",
      "Epoch 7/50 Threshold: 0.8001748318916596 Length of Training words: 2442030\n",
      "Global Step: 14000 Epoch 7/50 Iteration: 14000 Avg. Training loss: 0.6244 0.0066 sec/batch\n",
      "Global Step: 14100 Epoch 7/50 Iteration: 14100 Avg. Training loss: 0.5054 0.0064 sec/batch\n",
      "Global Step: 14200 Epoch 7/50 Iteration: 14200 Avg. Training loss: 0.5234 0.0063 sec/batch\n",
      "Global Step: 14300 Epoch 7/50 Iteration: 14300 Avg. Training loss: 0.5418 0.0066 sec/batch\n",
      "Global Step: 14400 Epoch 7/50 Iteration: 14400 Avg. Training loss: 0.5306 0.0067 sec/batch\n",
      "Global Step: 14500 Epoch 7/50 Iteration: 14500 Avg. Training loss: 0.4702 0.0067 sec/batch\n",
      "Global Step: 14600 Epoch 7/50 Iteration: 14600 Avg. Training loss: 0.4976 0.0070 sec/batch\n",
      "Global Step: 14700 Epoch 7/50 Iteration: 14700 Avg. Training loss: 0.5241 0.0069 sec/batch\n",
      "Global Step: 14800 Epoch 7/50 Iteration: 14800 Avg. Training loss: 0.5047 0.0067 sec/batch\n",
      "Global Step: 14900 Epoch 7/50 Iteration: 14900 Avg. Training loss: 0.5854 0.0067 sec/batch\n",
      "Global Step: 15000 Epoch 7/50 Iteration: 15000 Avg. Training loss: 0.5730 0.0069 sec/batch\n",
      "Global Step: 15100 Epoch 7/50 Iteration: 15100 Avg. Training loss: 0.5729 0.0066 sec/batch\n",
      "Global Step: 15200 Epoch 7/50 Iteration: 15200 Avg. Training loss: 0.5641 0.0071 sec/batch\n",
      "Global Step: 15300 Epoch 7/50 Iteration: 15300 Avg. Training loss: 0.5088 0.0068 sec/batch\n",
      "Global Step: 15400 Epoch 7/50 Iteration: 15400 Avg. Training loss: 0.5097 0.0072 sec/batch\n",
      "Global Step: 15500 Epoch 7/50 Iteration: 15500 Avg. Training loss: 0.5505 0.0070 sec/batch\n",
      "Global Step: 15600 Epoch 7/50 Iteration: 15600 Avg. Training loss: 0.5697 0.0071 sec/batch\n",
      "Global Step: 15700 Epoch 7/50 Iteration: 15700 Avg. Training loss: 0.4517 0.0066 sec/batch\n",
      "Global Step: 15800 Epoch 7/50 Iteration: 15800 Avg. Training loss: 0.4908 0.0068 sec/batch\n",
      "Global Step: 15900 Epoch 7/50 Iteration: 15900 Avg. Training loss: 0.5854 0.0071 sec/batch\n",
      "Global Step: 16000 Epoch 7/50 Iteration: 16000 Avg. Training loss: 0.4618 0.0063 sec/batch\n",
      "Global Step: 16100 Epoch 7/50 Iteration: 16100 Avg. Training loss: 0.5559 0.0069 sec/batch\n",
      "Global Step: 16200 Epoch 7/50 Iteration: 16200 Avg. Training loss: 0.4758 0.0066 sec/batch\n",
      "Global Step: 16300 Epoch 7/50 Iteration: 16300 Avg. Training loss: 0.5346 0.0065 sec/batch\n",
      "Epoch 8/50 Threshold: 0.7191427921555942 Length of Training words: 2325460\n",
      "Global Step: 16400 Epoch 8/50 Iteration: 16400 Avg. Training loss: 0.6045 0.0035 sec/batch\n",
      "Global Step: 16500 Epoch 8/50 Iteration: 16500 Avg. Training loss: 0.5706 0.0067 sec/batch\n",
      "Global Step: 16600 Epoch 8/50 Iteration: 16600 Avg. Training loss: 0.5070 0.0067 sec/batch\n",
      "Global Step: 16700 Epoch 8/50 Iteration: 16700 Avg. Training loss: 0.5906 0.0069 sec/batch\n",
      "Global Step: 16800 Epoch 8/50 Iteration: 16800 Avg. Training loss: 0.5485 0.0068 sec/batch\n",
      "Global Step: 16900 Epoch 8/50 Iteration: 16900 Avg. Training loss: 0.4799 0.0066 sec/batch\n",
      "Global Step: 17000 Epoch 8/50 Iteration: 17000 Avg. Training loss: 0.5122 0.0078 sec/batch\n",
      "Global Step: 17100 Epoch 8/50 Iteration: 17100 Avg. Training loss: 0.5309 0.0092 sec/batch\n",
      "Global Step: 17200 Epoch 8/50 Iteration: 17200 Avg. Training loss: 0.5253 0.0066 sec/batch\n",
      "Global Step: 17300 Epoch 8/50 Iteration: 17300 Avg. Training loss: 0.6011 0.0066 sec/batch\n",
      "Global Step: 17400 Epoch 8/50 Iteration: 17400 Avg. Training loss: 0.5826 0.0080 sec/batch\n",
      "Global Step: 17500 Epoch 8/50 Iteration: 17500 Avg. Training loss: 0.5781 0.0085 sec/batch\n",
      "Global Step: 17600 Epoch 8/50 Iteration: 17600 Avg. Training loss: 0.5809 0.0084 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 17700 Epoch 8/50 Iteration: 17700 Avg. Training loss: 0.5470 0.0077 sec/batch\n",
      "Global Step: 17800 Epoch 8/50 Iteration: 17800 Avg. Training loss: 0.5310 0.0069 sec/batch\n",
      "Global Step: 17900 Epoch 8/50 Iteration: 17900 Avg. Training loss: 0.5200 0.0073 sec/batch\n",
      "Global Step: 18000 Epoch 8/50 Iteration: 18000 Avg. Training loss: 0.5511 0.0073 sec/batch\n",
      "Global Step: 18100 Epoch 8/50 Iteration: 18100 Avg. Training loss: 0.5150 0.0066 sec/batch\n",
      "Global Step: 18200 Epoch 8/50 Iteration: 18200 Avg. Training loss: 0.5415 0.0064 sec/batch\n",
      "Global Step: 18300 Epoch 8/50 Iteration: 18300 Avg. Training loss: 0.5408 0.0086 sec/batch\n",
      "Global Step: 18400 Epoch 8/50 Iteration: 18400 Avg. Training loss: 0.4861 0.0088 sec/batch\n",
      "Global Step: 18500 Epoch 8/50 Iteration: 18500 Avg. Training loss: 0.5898 0.0082 sec/batch\n",
      "Global Step: 18600 Epoch 8/50 Iteration: 18600 Avg. Training loss: 0.4911 0.0083 sec/batch\n",
      "Epoch 9/50 Threshold: 0.7716715552212245 Length of Training words: 2401683\n",
      "Global Step: 18700 Epoch 9/50 Iteration: 18700 Avg. Training loss: 0.5933 0.0031 sec/batch\n",
      "Global Step: 18800 Epoch 9/50 Iteration: 18800 Avg. Training loss: 0.5936 0.0075 sec/batch\n",
      "Global Step: 18900 Epoch 9/50 Iteration: 18900 Avg. Training loss: 0.4972 0.0064 sec/batch\n",
      "Global Step: 19000 Epoch 9/50 Iteration: 19000 Avg. Training loss: 0.5830 0.0080 sec/batch\n",
      "Global Step: 19100 Epoch 9/50 Iteration: 19100 Avg. Training loss: 0.5171 0.0069 sec/batch\n",
      "Global Step: 19200 Epoch 9/50 Iteration: 19200 Avg. Training loss: 0.4942 0.0065 sec/batch\n",
      "Global Step: 19300 Epoch 9/50 Iteration: 19300 Avg. Training loss: 0.5148 0.0066 sec/batch\n",
      "Global Step: 19400 Epoch 9/50 Iteration: 19400 Avg. Training loss: 0.4723 0.0066 sec/batch\n",
      "Global Step: 19500 Epoch 9/50 Iteration: 19500 Avg. Training loss: 0.5696 0.0070 sec/batch\n",
      "Global Step: 19600 Epoch 9/50 Iteration: 19600 Avg. Training loss: 0.4989 0.0096 sec/batch\n",
      "Global Step: 19700 Epoch 9/50 Iteration: 19700 Avg. Training loss: 0.5884 0.0129 sec/batch\n",
      "Global Step: 19800 Epoch 9/50 Iteration: 19800 Avg. Training loss: 0.6008 0.0092 sec/batch\n",
      "Global Step: 19900 Epoch 9/50 Iteration: 19900 Avg. Training loss: 0.5714 0.0095 sec/batch\n",
      "Global Step: 20000 Epoch 9/50 Iteration: 20000 Avg. Training loss: 0.5630 0.0086 sec/batch\n",
      "Global Step: 20100 Epoch 9/50 Iteration: 20100 Avg. Training loss: 0.4944 0.0070 sec/batch\n",
      "Global Step: 20200 Epoch 9/50 Iteration: 20200 Avg. Training loss: 0.5383 0.0089 sec/batch\n",
      "Global Step: 20300 Epoch 9/50 Iteration: 20300 Avg. Training loss: 0.5418 0.0075 sec/batch\n",
      "Global Step: 20400 Epoch 9/50 Iteration: 20400 Avg. Training loss: 0.5367 0.0087 sec/batch\n",
      "Global Step: 20500 Epoch 9/50 Iteration: 20500 Avg. Training loss: 0.4770 0.0094 sec/batch\n",
      "Global Step: 20600 Epoch 9/50 Iteration: 20600 Avg. Training loss: 0.5630 0.0064 sec/batch\n",
      "Global Step: 20700 Epoch 9/50 Iteration: 20700 Avg. Training loss: 0.5014 0.0069 sec/batch\n",
      "Global Step: 20800 Epoch 9/50 Iteration: 20800 Avg. Training loss: 0.4886 0.0067 sec/batch\n",
      "Global Step: 20900 Epoch 9/50 Iteration: 20900 Avg. Training loss: 0.5797 0.0063 sec/batch\n",
      "Global Step: 21000 Epoch 9/50 Iteration: 21000 Avg. Training loss: 0.4733 0.0068 sec/batch\n",
      "Epoch 10/50 Threshold: 0.6715831276339495 Length of Training words: 2253990\n",
      "Global Step: 21100 Epoch 10/50 Iteration: 21100 Avg. Training loss: 0.5880 0.0020 sec/batch\n",
      "Global Step: 21200 Epoch 10/50 Iteration: 21200 Avg. Training loss: 0.6111 0.0069 sec/batch\n",
      "Global Step: 21300 Epoch 10/50 Iteration: 21300 Avg. Training loss: 0.5013 0.0077 sec/batch\n",
      "Global Step: 21400 Epoch 10/50 Iteration: 21400 Avg. Training loss: 0.6045 0.0091 sec/batch\n",
      "Global Step: 21500 Epoch 10/50 Iteration: 21500 Avg. Training loss: 0.5471 0.0074 sec/batch\n",
      "Global Step: 21600 Epoch 10/50 Iteration: 21600 Avg. Training loss: 0.4933 0.0065 sec/batch\n",
      "Global Step: 21700 Epoch 10/50 Iteration: 21700 Avg. Training loss: 0.5191 0.0067 sec/batch\n",
      "Global Step: 21800 Epoch 10/50 Iteration: 21800 Avg. Training loss: 0.5546 0.0089 sec/batch\n",
      "Global Step: 21900 Epoch 10/50 Iteration: 21900 Avg. Training loss: 0.5413 0.0107 sec/batch\n",
      "Global Step: 22000 Epoch 10/50 Iteration: 22000 Avg. Training loss: 0.6127 0.0087 sec/batch\n",
      "Global Step: 22100 Epoch 10/50 Iteration: 22100 Avg. Training loss: 0.5771 0.0106 sec/batch\n",
      "Global Step: 22200 Epoch 10/50 Iteration: 22200 Avg. Training loss: 0.6095 0.0074 sec/batch\n",
      "Global Step: 22300 Epoch 10/50 Iteration: 22300 Avg. Training loss: 0.5943 0.0091 sec/batch\n",
      "Global Step: 22400 Epoch 10/50 Iteration: 22400 Avg. Training loss: 0.5297 0.0087 sec/batch\n",
      "Global Step: 22500 Epoch 10/50 Iteration: 22500 Avg. Training loss: 0.5487 0.0074 sec/batch\n",
      "Global Step: 22600 Epoch 10/50 Iteration: 22600 Avg. Training loss: 0.5509 0.0083 sec/batch\n",
      "Global Step: 22700 Epoch 10/50 Iteration: 22700 Avg. Training loss: 0.5507 0.0082 sec/batch\n",
      "Global Step: 22800 Epoch 10/50 Iteration: 22800 Avg. Training loss: 0.5077 0.0083 sec/batch\n",
      "Global Step: 22900 Epoch 10/50 Iteration: 22900 Avg. Training loss: 0.6009 0.0064 sec/batch\n",
      "Global Step: 23000 Epoch 10/50 Iteration: 23000 Avg. Training loss: 0.5027 0.0067 sec/batch\n",
      "Global Step: 23100 Epoch 10/50 Iteration: 23100 Avg. Training loss: 0.5538 0.0062 sec/batch\n",
      "Global Step: 23200 Epoch 10/50 Iteration: 23200 Avg. Training loss: 0.5127 0.0062 sec/batch\n",
      "Global Step: 23300 Epoch 10/50 Iteration: 23300 Avg. Training loss: 0.5702 0.0065 sec/batch\n",
      "Epoch 11/50 Threshold: 0.8101392187828012 Length of Training words: 2455516\n",
      "Global Step: 23400 Epoch 11/50 Iteration: 23400 Avg. Training loss: 0.5891 0.0053 sec/batch\n",
      "Global Step: 23500 Epoch 11/50 Iteration: 23500 Avg. Training loss: 0.5348 0.0067 sec/batch\n",
      "Global Step: 23600 Epoch 11/50 Iteration: 23600 Avg. Training loss: 0.5138 0.0066 sec/batch\n",
      "Global Step: 23700 Epoch 11/50 Iteration: 23700 Avg. Training loss: 0.5631 0.0068 sec/batch\n",
      "Global Step: 23800 Epoch 11/50 Iteration: 23800 Avg. Training loss: 0.5343 0.0068 sec/batch\n",
      "Global Step: 23900 Epoch 11/50 Iteration: 23900 Avg. Training loss: 0.4420 0.0069 sec/batch\n",
      "Global Step: 24000 Epoch 11/50 Iteration: 24000 Avg. Training loss: 0.4996 0.0067 sec/batch\n",
      "Global Step: 24100 Epoch 11/50 Iteration: 24100 Avg. Training loss: 0.4755 0.0063 sec/batch\n",
      "Global Step: 24200 Epoch 11/50 Iteration: 24200 Avg. Training loss: 0.5653 0.0067 sec/batch\n",
      "Global Step: 24300 Epoch 11/50 Iteration: 24300 Avg. Training loss: 0.5453 0.0067 sec/batch\n",
      "Global Step: 24400 Epoch 11/50 Iteration: 24400 Avg. Training loss: 0.5832 0.0065 sec/batch\n",
      "Global Step: 24500 Epoch 11/50 Iteration: 24500 Avg. Training loss: 0.5707 0.0066 sec/batch\n",
      "Global Step: 24600 Epoch 11/50 Iteration: 24600 Avg. Training loss: 0.5399 0.0064 sec/batch\n",
      "Global Step: 24700 Epoch 11/50 Iteration: 24700 Avg. Training loss: 0.5646 0.0070 sec/batch\n",
      "Global Step: 24800 Epoch 11/50 Iteration: 24800 Avg. Training loss: 0.4877 0.0066 sec/batch\n",
      "Global Step: 24900 Epoch 11/50 Iteration: 24900 Avg. Training loss: 0.5352 0.0064 sec/batch\n",
      "Global Step: 25000 Epoch 11/50 Iteration: 25000 Avg. Training loss: 0.5506 0.0069 sec/batch\n",
      "Global Step: 25100 Epoch 11/50 Iteration: 25100 Avg. Training loss: 0.5076 0.0065 sec/batch\n",
      "Global Step: 25200 Epoch 11/50 Iteration: 25200 Avg. Training loss: 0.4861 0.0067 sec/batch\n",
      "Global Step: 25300 Epoch 11/50 Iteration: 25300 Avg. Training loss: 0.5538 0.0067 sec/batch\n",
      "Global Step: 25400 Epoch 11/50 Iteration: 25400 Avg. Training loss: 0.4918 0.0063 sec/batch\n",
      "Global Step: 25500 Epoch 11/50 Iteration: 25500 Avg. Training loss: 0.4798 0.0064 sec/batch\n",
      "Global Step: 25600 Epoch 11/50 Iteration: 25600 Avg. Training loss: 0.5640 0.0065 sec/batch\n",
      "Global Step: 25700 Epoch 11/50 Iteration: 25700 Avg. Training loss: 0.4701 0.0063 sec/batch\n",
      "Epoch 12/50 Threshold: 0.8249418060788323 Length of Training words: 2476076\n",
      "Global Step: 25800 Epoch 12/50 Iteration: 25800 Avg. Training loss: 0.5728 0.0014 sec/batch\n",
      "Global Step: 25900 Epoch 12/50 Iteration: 25900 Avg. Training loss: 0.5865 0.0066 sec/batch\n",
      "Global Step: 26000 Epoch 12/50 Iteration: 26000 Avg. Training loss: 0.4962 0.0069 sec/batch\n",
      "Global Step: 26100 Epoch 12/50 Iteration: 26100 Avg. Training loss: 0.5502 0.0069 sec/batch\n",
      "Global Step: 26200 Epoch 12/50 Iteration: 26200 Avg. Training loss: 0.5391 0.0070 sec/batch\n",
      "Global Step: 26300 Epoch 12/50 Iteration: 26300 Avg. Training loss: 0.4810 0.0064 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 26400 Epoch 12/50 Iteration: 26400 Avg. Training loss: 0.4929 0.0066 sec/batch\n",
      "Global Step: 26500 Epoch 12/50 Iteration: 26500 Avg. Training loss: 0.4912 0.0066 sec/batch\n",
      "Global Step: 26600 Epoch 12/50 Iteration: 26600 Avg. Training loss: 0.5161 0.0065 sec/batch\n",
      "Global Step: 26700 Epoch 12/50 Iteration: 26700 Avg. Training loss: 0.5067 0.0070 sec/batch\n",
      "Global Step: 26800 Epoch 12/50 Iteration: 26800 Avg. Training loss: 0.5850 0.0065 sec/batch\n",
      "Global Step: 26900 Epoch 12/50 Iteration: 26900 Avg. Training loss: 0.5679 0.0065 sec/batch\n",
      "Global Step: 27000 Epoch 12/50 Iteration: 27000 Avg. Training loss: 0.5550 0.0065 sec/batch\n",
      "Global Step: 27100 Epoch 12/50 Iteration: 27100 Avg. Training loss: 0.5714 0.0065 sec/batch\n",
      "Global Step: 27200 Epoch 12/50 Iteration: 27200 Avg. Training loss: 0.5088 0.0069 sec/batch\n",
      "Global Step: 27300 Epoch 12/50 Iteration: 27300 Avg. Training loss: 0.5100 0.0067 sec/batch\n",
      "Global Step: 27400 Epoch 12/50 Iteration: 27400 Avg. Training loss: 0.5294 0.0067 sec/batch\n",
      "Global Step: 27500 Epoch 12/50 Iteration: 27500 Avg. Training loss: 0.5666 0.0077 sec/batch\n",
      "Global Step: 27600 Epoch 12/50 Iteration: 27600 Avg. Training loss: 0.4417 0.0086 sec/batch\n",
      "Global Step: 27700 Epoch 12/50 Iteration: 27700 Avg. Training loss: 0.4851 0.0096 sec/batch\n",
      "Global Step: 27800 Epoch 12/50 Iteration: 27800 Avg. Training loss: 0.5869 0.0075 sec/batch\n",
      "Global Step: 27900 Epoch 12/50 Iteration: 27900 Avg. Training loss: 0.4767 0.0068 sec/batch\n",
      "Global Step: 28000 Epoch 12/50 Iteration: 28000 Avg. Training loss: 0.5193 0.0064 sec/batch\n",
      "Global Step: 28100 Epoch 12/50 Iteration: 28100 Avg. Training loss: 0.5095 0.0069 sec/batch\n",
      "Global Step: 28200 Epoch 12/50 Iteration: 28200 Avg. Training loss: 0.4901 0.0068 sec/batch\n",
      "Epoch 13/50 Threshold: 0.6588146017053153 Length of Training words: 2233663\n",
      "Global Step: 28300 Epoch 13/50 Iteration: 28300 Avg. Training loss: 0.5999 0.0046 sec/batch\n",
      "Global Step: 28400 Epoch 13/50 Iteration: 28400 Avg. Training loss: 0.6007 0.0068 sec/batch\n",
      "Global Step: 28500 Epoch 13/50 Iteration: 28500 Avg. Training loss: 0.5202 0.0070 sec/batch\n",
      "Global Step: 28600 Epoch 13/50 Iteration: 28600 Avg. Training loss: 0.5999 0.0068 sec/batch\n",
      "Global Step: 28700 Epoch 13/50 Iteration: 28700 Avg. Training loss: 0.5610 0.0072 sec/batch\n",
      "Global Step: 28800 Epoch 13/50 Iteration: 28800 Avg. Training loss: 0.4867 0.0068 sec/batch\n",
      "Global Step: 28900 Epoch 13/50 Iteration: 28900 Avg. Training loss: 0.5335 0.0084 sec/batch\n",
      "Global Step: 29000 Epoch 13/50 Iteration: 29000 Avg. Training loss: 0.5374 0.0067 sec/batch\n",
      "Global Step: 29100 Epoch 13/50 Iteration: 29100 Avg. Training loss: 0.5500 0.0062 sec/batch\n",
      "Global Step: 29200 Epoch 13/50 Iteration: 29200 Avg. Training loss: 0.6230 0.0074 sec/batch\n",
      "Global Step: 29300 Epoch 13/50 Iteration: 29300 Avg. Training loss: 0.5926 0.0082 sec/batch\n",
      "Global Step: 29400 Epoch 13/50 Iteration: 29400 Avg. Training loss: 0.5991 0.0086 sec/batch\n",
      "Global Step: 29500 Epoch 13/50 Iteration: 29500 Avg. Training loss: 0.5847 0.0079 sec/batch\n",
      "Global Step: 29600 Epoch 13/50 Iteration: 29600 Avg. Training loss: 0.5116 0.0064 sec/batch\n",
      "Global Step: 29700 Epoch 13/50 Iteration: 29700 Avg. Training loss: 0.5724 0.0066 sec/batch\n",
      "Global Step: 29800 Epoch 13/50 Iteration: 29800 Avg. Training loss: 0.5986 0.0065 sec/batch\n",
      "Global Step: 29900 Epoch 13/50 Iteration: 29900 Avg. Training loss: 0.4885 0.0067 sec/batch\n",
      "Global Step: 30000 Epoch 13/50 Iteration: 30000 Avg. Training loss: 0.5332 0.0090 sec/batch\n",
      "Global Step: 30100 Epoch 13/50 Iteration: 30100 Avg. Training loss: 0.5921 0.0076 sec/batch\n",
      "Global Step: 30200 Epoch 13/50 Iteration: 30200 Avg. Training loss: 0.4947 0.0069 sec/batch\n",
      "Global Step: 30300 Epoch 13/50 Iteration: 30300 Avg. Training loss: 0.5733 0.0069 sec/batch\n",
      "Global Step: 30400 Epoch 13/50 Iteration: 30400 Avg. Training loss: 0.5073 0.0069 sec/batch\n",
      "Epoch 14/50 Threshold: 0.7814153129454683 Length of Training words: 2415144\n",
      "Global Step: 30500 Epoch 14/50 Iteration: 30500 Avg. Training loss: 0.5864 0.0008 sec/batch\n",
      "Global Step: 30600 Epoch 14/50 Iteration: 30600 Avg. Training loss: 0.6084 0.0070 sec/batch\n",
      "Global Step: 30700 Epoch 14/50 Iteration: 30700 Avg. Training loss: 0.4979 0.0068 sec/batch\n",
      "Global Step: 30800 Epoch 14/50 Iteration: 30800 Avg. Training loss: 0.5605 0.0069 sec/batch\n",
      "Global Step: 30900 Epoch 14/50 Iteration: 30900 Avg. Training loss: 0.5476 0.0074 sec/batch\n",
      "Global Step: 31000 Epoch 14/50 Iteration: 31000 Avg. Training loss: 0.4919 0.0067 sec/batch\n",
      "Global Step: 31100 Epoch 14/50 Iteration: 31100 Avg. Training loss: 0.5085 0.0073 sec/batch\n",
      "Global Step: 31200 Epoch 14/50 Iteration: 31200 Avg. Training loss: 0.4944 0.0089 sec/batch\n",
      "Global Step: 31300 Epoch 14/50 Iteration: 31300 Avg. Training loss: 0.5200 0.0069 sec/batch\n",
      "Global Step: 31400 Epoch 14/50 Iteration: 31400 Avg. Training loss: 0.5225 0.0070 sec/batch\n",
      "Global Step: 31500 Epoch 14/50 Iteration: 31500 Avg. Training loss: 0.6044 0.0066 sec/batch\n",
      "Global Step: 31600 Epoch 14/50 Iteration: 31600 Avg. Training loss: 0.5369 0.0072 sec/batch\n",
      "Global Step: 31700 Epoch 14/50 Iteration: 31700 Avg. Training loss: 0.5996 0.0069 sec/batch\n",
      "Global Step: 31800 Epoch 14/50 Iteration: 31800 Avg. Training loss: 0.5649 0.0082 sec/batch\n",
      "Global Step: 31900 Epoch 14/50 Iteration: 31900 Avg. Training loss: 0.5345 0.0070 sec/batch\n",
      "Global Step: 32000 Epoch 14/50 Iteration: 32000 Avg. Training loss: 0.5120 0.0080 sec/batch\n",
      "Global Step: 32100 Epoch 14/50 Iteration: 32100 Avg. Training loss: 0.5089 0.0078 sec/batch\n",
      "Global Step: 32200 Epoch 14/50 Iteration: 32200 Avg. Training loss: 0.5241 0.0083 sec/batch\n",
      "Global Step: 32300 Epoch 14/50 Iteration: 32300 Avg. Training loss: 0.5215 0.0077 sec/batch\n",
      "Global Step: 32400 Epoch 14/50 Iteration: 32400 Avg. Training loss: 0.5162 0.0076 sec/batch\n",
      "Global Step: 32500 Epoch 14/50 Iteration: 32500 Avg. Training loss: 0.5546 0.0083 sec/batch\n",
      "Global Step: 32600 Epoch 14/50 Iteration: 32600 Avg. Training loss: 0.4716 0.0085 sec/batch\n",
      "Global Step: 32700 Epoch 14/50 Iteration: 32700 Avg. Training loss: 0.5512 0.0080 sec/batch\n",
      "Global Step: 32800 Epoch 14/50 Iteration: 32800 Avg. Training loss: 0.4938 0.0091 sec/batch\n",
      "Global Step: 32900 Epoch 14/50 Iteration: 32900 Avg. Training loss: 0.5406 0.0091 sec/batch\n",
      "Epoch 15/50 Threshold: 0.8992026848395152 Length of Training words: 2576456\n",
      "Global Step: 33000 Epoch 15/50 Iteration: 33000 Avg. Training loss: 0.6051 0.0081 sec/batch\n",
      "Global Step: 33100 Epoch 15/50 Iteration: 33100 Avg. Training loss: 0.4956 0.0099 sec/batch\n",
      "Global Step: 33200 Epoch 15/50 Iteration: 33200 Avg. Training loss: 0.5077 0.0122 sec/batch\n",
      "Global Step: 33300 Epoch 15/50 Iteration: 33300 Avg. Training loss: 0.5430 0.0096 sec/batch\n",
      "Global Step: 33400 Epoch 15/50 Iteration: 33400 Avg. Training loss: 0.5181 0.0068 sec/batch\n",
      "Global Step: 33500 Epoch 15/50 Iteration: 33500 Avg. Training loss: 0.4413 0.0071 sec/batch\n",
      "Global Step: 33600 Epoch 15/50 Iteration: 33600 Avg. Training loss: 0.4733 0.0068 sec/batch\n",
      "Global Step: 33700 Epoch 15/50 Iteration: 33700 Avg. Training loss: 0.4607 0.0063 sec/batch\n",
      "Global Step: 33800 Epoch 15/50 Iteration: 33800 Avg. Training loss: 0.5550 0.0065 sec/batch\n",
      "Global Step: 33900 Epoch 15/50 Iteration: 33900 Avg. Training loss: 0.4842 0.0067 sec/batch\n",
      "Global Step: 34000 Epoch 15/50 Iteration: 34000 Avg. Training loss: 0.5805 0.0074 sec/batch\n",
      "Global Step: 34100 Epoch 15/50 Iteration: 34100 Avg. Training loss: 0.5490 0.0064 sec/batch\n",
      "Global Step: 34200 Epoch 15/50 Iteration: 34200 Avg. Training loss: 0.5785 0.0066 sec/batch\n",
      "Global Step: 34300 Epoch 15/50 Iteration: 34300 Avg. Training loss: 0.5332 0.0066 sec/batch\n",
      "Global Step: 34400 Epoch 15/50 Iteration: 34400 Avg. Training loss: 0.5054 0.0068 sec/batch\n",
      "Global Step: 34500 Epoch 15/50 Iteration: 34500 Avg. Training loss: 0.4865 0.0067 sec/batch\n",
      "Global Step: 34600 Epoch 15/50 Iteration: 34600 Avg. Training loss: 0.5009 0.0068 sec/batch\n",
      "Global Step: 34700 Epoch 15/50 Iteration: 34700 Avg. Training loss: 0.5684 0.0060 sec/batch\n",
      "Global Step: 34800 Epoch 15/50 Iteration: 34800 Avg. Training loss: 0.4393 0.0064 sec/batch\n",
      "Global Step: 34900 Epoch 15/50 Iteration: 34900 Avg. Training loss: 0.4678 0.0066 sec/batch\n",
      "Global Step: 35000 Epoch 15/50 Iteration: 35000 Avg. Training loss: 0.5907 0.0066 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 35100 Epoch 15/50 Iteration: 35100 Avg. Training loss: 0.4474 0.0068 sec/batch\n",
      "Global Step: 35200 Epoch 15/50 Iteration: 35200 Avg. Training loss: 0.4796 0.0067 sec/batch\n",
      "Global Step: 35300 Epoch 15/50 Iteration: 35300 Avg. Training loss: 0.5526 0.0065 sec/batch\n",
      "Global Step: 35400 Epoch 15/50 Iteration: 35400 Avg. Training loss: 0.4471 0.0065 sec/batch\n",
      "Epoch 16/50 Threshold: 0.7760363353058254 Length of Training words: 2407919\n",
      "Global Step: 35500 Epoch 16/50 Iteration: 35500 Avg. Training loss: 0.5610 0.0014 sec/batch\n",
      "Global Step: 35600 Epoch 16/50 Iteration: 35600 Avg. Training loss: 0.5989 0.0067 sec/batch\n",
      "Global Step: 35700 Epoch 16/50 Iteration: 35700 Avg. Training loss: 0.5055 0.0065 sec/batch\n",
      "Global Step: 35800 Epoch 16/50 Iteration: 35800 Avg. Training loss: 0.5647 0.0066 sec/batch\n",
      "Global Step: 35900 Epoch 16/50 Iteration: 35900 Avg. Training loss: 0.5518 0.0066 sec/batch\n",
      "Global Step: 36000 Epoch 16/50 Iteration: 36000 Avg. Training loss: 0.4939 0.0073 sec/batch\n",
      "Global Step: 36100 Epoch 16/50 Iteration: 36100 Avg. Training loss: 0.5013 0.0074 sec/batch\n",
      "Global Step: 36200 Epoch 16/50 Iteration: 36200 Avg. Training loss: 0.4816 0.0065 sec/batch\n",
      "Global Step: 36300 Epoch 16/50 Iteration: 36300 Avg. Training loss: 0.5421 0.0065 sec/batch\n",
      "Global Step: 36400 Epoch 16/50 Iteration: 36400 Avg. Training loss: 0.4928 0.0068 sec/batch\n",
      "Global Step: 36500 Epoch 16/50 Iteration: 36500 Avg. Training loss: 0.6083 0.0064 sec/batch\n",
      "Global Step: 36600 Epoch 16/50 Iteration: 36600 Avg. Training loss: 0.5656 0.0068 sec/batch\n",
      "Global Step: 36700 Epoch 16/50 Iteration: 36700 Avg. Training loss: 0.5949 0.0066 sec/batch\n",
      "Global Step: 36800 Epoch 16/50 Iteration: 36800 Avg. Training loss: 0.5667 0.0069 sec/batch\n",
      "Global Step: 36900 Epoch 16/50 Iteration: 36900 Avg. Training loss: 0.5021 0.0067 sec/batch\n",
      "Global Step: 37000 Epoch 16/50 Iteration: 37000 Avg. Training loss: 0.5375 0.0091 sec/batch\n",
      "Global Step: 37100 Epoch 16/50 Iteration: 37100 Avg. Training loss: 0.5105 0.0069 sec/batch\n",
      "Global Step: 37200 Epoch 16/50 Iteration: 37200 Avg. Training loss: 0.5451 0.0070 sec/batch\n",
      "Global Step: 37300 Epoch 16/50 Iteration: 37300 Avg. Training loss: 0.4825 0.0066 sec/batch\n",
      "Global Step: 37400 Epoch 16/50 Iteration: 37400 Avg. Training loss: 0.5370 0.0067 sec/batch\n",
      "Global Step: 37500 Epoch 16/50 Iteration: 37500 Avg. Training loss: 0.5310 0.0077 sec/batch\n",
      "Global Step: 37600 Epoch 16/50 Iteration: 37600 Avg. Training loss: 0.4751 0.0078 sec/batch\n",
      "Global Step: 37700 Epoch 16/50 Iteration: 37700 Avg. Training loss: 0.5659 0.0084 sec/batch\n",
      "Global Step: 37800 Epoch 16/50 Iteration: 37800 Avg. Training loss: 0.4816 0.0079 sec/batch\n",
      "Epoch 17/50 Threshold: 0.6425010341448976 Length of Training words: 2208208\n",
      "Global Step: 37900 Epoch 17/50 Iteration: 37900 Avg. Training loss: 0.5728 0.0011 sec/batch\n",
      "Global Step: 38000 Epoch 17/50 Iteration: 38000 Avg. Training loss: 0.6285 0.0083 sec/batch\n",
      "Global Step: 38100 Epoch 17/50 Iteration: 38100 Avg. Training loss: 0.5287 0.0110 sec/batch\n",
      "Global Step: 38200 Epoch 17/50 Iteration: 38200 Avg. Training loss: 0.6034 0.0104 sec/batch\n",
      "Global Step: 38300 Epoch 17/50 Iteration: 38300 Avg. Training loss: 0.5441 0.0082 sec/batch\n",
      "Global Step: 38400 Epoch 17/50 Iteration: 38400 Avg. Training loss: 0.5032 0.0080 sec/batch\n",
      "Global Step: 38500 Epoch 17/50 Iteration: 38500 Avg. Training loss: 0.5256 0.0078 sec/batch\n",
      "Global Step: 38600 Epoch 17/50 Iteration: 38600 Avg. Training loss: 0.5532 0.0077 sec/batch\n",
      "Global Step: 38700 Epoch 17/50 Iteration: 38700 Avg. Training loss: 0.5484 0.0104 sec/batch\n",
      "Global Step: 38800 Epoch 17/50 Iteration: 38800 Avg. Training loss: 0.6263 0.0086 sec/batch\n",
      "Global Step: 38900 Epoch 17/50 Iteration: 38900 Avg. Training loss: 0.5730 0.0109 sec/batch\n",
      "Global Step: 39000 Epoch 17/50 Iteration: 39000 Avg. Training loss: 0.6354 0.0073 sec/batch\n",
      "Global Step: 39100 Epoch 17/50 Iteration: 39100 Avg. Training loss: 0.5857 0.0067 sec/batch\n",
      "Global Step: 39200 Epoch 17/50 Iteration: 39200 Avg. Training loss: 0.5217 0.0066 sec/batch\n",
      "Global Step: 39300 Epoch 17/50 Iteration: 39300 Avg. Training loss: 0.5569 0.0079 sec/batch\n",
      "Global Step: 39400 Epoch 17/50 Iteration: 39400 Avg. Training loss: 0.5995 0.0096 sec/batch\n",
      "Global Step: 39500 Epoch 17/50 Iteration: 39500 Avg. Training loss: 0.4917 0.0095 sec/batch\n",
      "Global Step: 39600 Epoch 17/50 Iteration: 39600 Avg. Training loss: 0.5253 0.0081 sec/batch\n",
      "Global Step: 39700 Epoch 17/50 Iteration: 39700 Avg. Training loss: 0.6117 0.0089 sec/batch\n",
      "Global Step: 39800 Epoch 17/50 Iteration: 39800 Avg. Training loss: 0.5085 0.0073 sec/batch\n",
      "Global Step: 39900 Epoch 17/50 Iteration: 39900 Avg. Training loss: 0.5762 0.0071 sec/batch\n",
      "Global Step: 40000 Epoch 17/50 Iteration: 40000 Avg. Training loss: 0.5130 0.0068 sec/batch\n",
      "Epoch 18/50 Threshold: 0.7187130317117564 Length of Training words: 2324214\n",
      "Global Step: 40100 Epoch 18/50 Iteration: 40100 Avg. Training loss: 0.5760 0.0006 sec/batch\n",
      "Global Step: 40200 Epoch 18/50 Iteration: 40200 Avg. Training loss: 0.6235 0.0096 sec/batch\n",
      "Global Step: 40300 Epoch 18/50 Iteration: 40300 Avg. Training loss: 0.5124 0.0075 sec/batch\n",
      "Global Step: 40400 Epoch 18/50 Iteration: 40400 Avg. Training loss: 0.5691 0.0086 sec/batch\n",
      "Global Step: 40500 Epoch 18/50 Iteration: 40500 Avg. Training loss: 0.5650 0.0088 sec/batch\n",
      "Global Step: 40600 Epoch 18/50 Iteration: 40600 Avg. Training loss: 0.5048 0.0087 sec/batch\n",
      "Global Step: 40700 Epoch 18/50 Iteration: 40700 Avg. Training loss: 0.5103 0.0081 sec/batch\n",
      "Global Step: 40800 Epoch 18/50 Iteration: 40800 Avg. Training loss: 0.4787 0.0074 sec/batch\n",
      "Global Step: 40900 Epoch 18/50 Iteration: 40900 Avg. Training loss: 0.5731 0.0065 sec/batch\n",
      "Global Step: 41000 Epoch 18/50 Iteration: 41000 Avg. Training loss: 0.5206 0.0064 sec/batch\n",
      "Global Step: 41100 Epoch 18/50 Iteration: 41100 Avg. Training loss: 0.6013 0.0074 sec/batch\n",
      "Global Step: 41200 Epoch 18/50 Iteration: 41200 Avg. Training loss: 0.6103 0.0067 sec/batch\n",
      "Global Step: 41300 Epoch 18/50 Iteration: 41300 Avg. Training loss: 0.5570 0.0066 sec/batch\n",
      "Global Step: 41400 Epoch 18/50 Iteration: 41400 Avg. Training loss: 0.5736 0.0066 sec/batch\n",
      "Global Step: 41500 Epoch 18/50 Iteration: 41500 Avg. Training loss: 0.5134 0.0072 sec/batch\n",
      "Global Step: 41600 Epoch 18/50 Iteration: 41600 Avg. Training loss: 0.5623 0.0067 sec/batch\n",
      "Global Step: 41700 Epoch 18/50 Iteration: 41700 Avg. Training loss: 0.5803 0.0085 sec/batch\n",
      "Global Step: 41800 Epoch 18/50 Iteration: 41800 Avg. Training loss: 0.4612 0.0069 sec/batch\n",
      "Global Step: 41900 Epoch 18/50 Iteration: 41900 Avg. Training loss: 0.5167 0.0074 sec/batch\n",
      "Global Step: 42000 Epoch 18/50 Iteration: 42000 Avg. Training loss: 0.5931 0.0104 sec/batch\n",
      "Global Step: 42100 Epoch 18/50 Iteration: 42100 Avg. Training loss: 0.4698 0.0083 sec/batch\n",
      "Global Step: 42200 Epoch 18/50 Iteration: 42200 Avg. Training loss: 0.5873 0.0065 sec/batch\n",
      "Global Step: 42300 Epoch 18/50 Iteration: 42300 Avg. Training loss: 0.4910 0.0078 sec/batch\n",
      "Global Step: 42400 Epoch 18/50 Iteration: 42400 Avg. Training loss: 0.5656 0.0092 sec/batch\n",
      "Epoch 19/50 Threshold: 0.7089781507072425 Length of Training words: 2309915\n",
      "Global Step: 42500 Epoch 19/50 Iteration: 42500 Avg. Training loss: 0.6086 0.0059 sec/batch\n",
      "Global Step: 42600 Epoch 19/50 Iteration: 42600 Avg. Training loss: 0.5410 0.0076 sec/batch\n",
      "Global Step: 42700 Epoch 19/50 Iteration: 42700 Avg. Training loss: 0.5382 0.0074 sec/batch\n",
      "Global Step: 42800 Epoch 19/50 Iteration: 42800 Avg. Training loss: 0.5659 0.0065 sec/batch\n",
      "Global Step: 42900 Epoch 19/50 Iteration: 42900 Avg. Training loss: 0.5305 0.0074 sec/batch\n",
      "Global Step: 43000 Epoch 19/50 Iteration: 43000 Avg. Training loss: 0.5035 0.0068 sec/batch\n",
      "Global Step: 43100 Epoch 19/50 Iteration: 43100 Avg. Training loss: 0.5141 0.0067 sec/batch\n",
      "Global Step: 43200 Epoch 19/50 Iteration: 43200 Avg. Training loss: 0.5397 0.0070 sec/batch\n",
      "Global Step: 43300 Epoch 19/50 Iteration: 43300 Avg. Training loss: 0.5135 0.0066 sec/batch\n",
      "Global Step: 43400 Epoch 19/50 Iteration: 43400 Avg. Training loss: 0.6155 0.0064 sec/batch\n",
      "Global Step: 43500 Epoch 19/50 Iteration: 43500 Avg. Training loss: 0.5910 0.0068 sec/batch\n",
      "Global Step: 43600 Epoch 19/50 Iteration: 43600 Avg. Training loss: 0.5874 0.0069 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 43700 Epoch 19/50 Iteration: 43700 Avg. Training loss: 0.5755 0.0078 sec/batch\n",
      "Global Step: 43800 Epoch 19/50 Iteration: 43800 Avg. Training loss: 0.5115 0.0071 sec/batch\n",
      "Global Step: 43900 Epoch 19/50 Iteration: 43900 Avg. Training loss: 0.5414 0.0064 sec/batch\n",
      "Global Step: 44000 Epoch 19/50 Iteration: 44000 Avg. Training loss: 0.5803 0.0066 sec/batch\n",
      "Global Step: 44100 Epoch 19/50 Iteration: 44100 Avg. Training loss: 0.4962 0.0063 sec/batch\n",
      "Global Step: 44200 Epoch 19/50 Iteration: 44200 Avg. Training loss: 0.5153 0.0065 sec/batch\n",
      "Global Step: 44300 Epoch 19/50 Iteration: 44300 Avg. Training loss: 0.5961 0.0068 sec/batch\n",
      "Global Step: 44400 Epoch 19/50 Iteration: 44400 Avg. Training loss: 0.4770 0.0067 sec/batch\n",
      "Global Step: 44500 Epoch 19/50 Iteration: 44500 Avg. Training loss: 0.5658 0.0074 sec/batch\n",
      "Global Step: 44600 Epoch 19/50 Iteration: 44600 Avg. Training loss: 0.4999 0.0072 sec/batch\n",
      "Global Step: 44700 Epoch 19/50 Iteration: 44700 Avg. Training loss: 0.5629 0.0069 sec/batch\n",
      "Epoch 20/50 Threshold: 0.620976471761424 Length of Training words: 2174775\n",
      "Global Step: 44800 Epoch 20/50 Iteration: 44800 Avg. Training loss: 0.6121 0.0047 sec/batch\n",
      "Global Step: 44900 Epoch 20/50 Iteration: 44900 Avg. Training loss: 0.5667 0.0069 sec/batch\n",
      "Global Step: 45000 Epoch 20/50 Iteration: 45000 Avg. Training loss: 0.5584 0.0068 sec/batch\n",
      "Global Step: 45100 Epoch 20/50 Iteration: 45100 Avg. Training loss: 0.5956 0.0066 sec/batch\n",
      "Global Step: 45200 Epoch 20/50 Iteration: 45200 Avg. Training loss: 0.5389 0.0068 sec/batch\n",
      "Global Step: 45300 Epoch 20/50 Iteration: 45300 Avg. Training loss: 0.5300 0.0067 sec/batch\n",
      "Global Step: 45400 Epoch 20/50 Iteration: 45400 Avg. Training loss: 0.4872 0.0065 sec/batch\n",
      "Global Step: 45500 Epoch 20/50 Iteration: 45500 Avg. Training loss: 0.5909 0.0068 sec/batch\n",
      "Global Step: 45600 Epoch 20/50 Iteration: 45600 Avg. Training loss: 0.5974 0.0065 sec/batch\n",
      "Global Step: 45700 Epoch 20/50 Iteration: 45700 Avg. Training loss: 0.6215 0.0066 sec/batch\n",
      "Global Step: 45800 Epoch 20/50 Iteration: 45800 Avg. Training loss: 0.5914 0.0066 sec/batch\n",
      "Global Step: 45900 Epoch 20/50 Iteration: 45900 Avg. Training loss: 0.5998 0.0067 sec/batch\n",
      "Global Step: 46000 Epoch 20/50 Iteration: 46000 Avg. Training loss: 0.5764 0.0064 sec/batch\n",
      "Global Step: 46100 Epoch 20/50 Iteration: 46100 Avg. Training loss: 0.5502 0.0065 sec/batch\n",
      "Global Step: 46200 Epoch 20/50 Iteration: 46200 Avg. Training loss: 0.5495 0.0069 sec/batch\n",
      "Global Step: 46300 Epoch 20/50 Iteration: 46300 Avg. Training loss: 0.5667 0.0066 sec/batch\n",
      "Global Step: 46400 Epoch 20/50 Iteration: 46400 Avg. Training loss: 0.5095 0.0069 sec/batch\n",
      "Global Step: 46500 Epoch 20/50 Iteration: 46500 Avg. Training loss: 0.6126 0.0068 sec/batch\n",
      "Global Step: 46600 Epoch 20/50 Iteration: 46600 Avg. Training loss: 0.4791 0.0062 sec/batch\n",
      "Global Step: 46700 Epoch 20/50 Iteration: 46700 Avg. Training loss: 0.6177 0.0071 sec/batch\n",
      "Global Step: 46800 Epoch 20/50 Iteration: 46800 Avg. Training loss: 0.5149 0.0069 sec/batch\n",
      "Global Step: 46900 Epoch 20/50 Iteration: 46900 Avg. Training loss: 0.5689 0.0065 sec/batch\n",
      "Epoch 21/50 Threshold: 0.6154677839268128 Length of Training words: 2165940\n",
      "Global Step: 47000 Epoch 21/50 Iteration: 47000 Avg. Training loss: 0.6455 0.0065 sec/batch\n",
      "Global Step: 47100 Epoch 21/50 Iteration: 47100 Avg. Training loss: 0.5390 0.0071 sec/batch\n",
      "Global Step: 47200 Epoch 21/50 Iteration: 47200 Avg. Training loss: 0.6103 0.0071 sec/batch\n",
      "Global Step: 47300 Epoch 21/50 Iteration: 47300 Avg. Training loss: 0.5366 0.0069 sec/batch\n",
      "Global Step: 47400 Epoch 21/50 Iteration: 47400 Avg. Training loss: 0.5275 0.0068 sec/batch\n",
      "Global Step: 47500 Epoch 21/50 Iteration: 47500 Avg. Training loss: 0.5317 0.0068 sec/batch\n",
      "Global Step: 47600 Epoch 21/50 Iteration: 47600 Avg. Training loss: 0.5450 0.0068 sec/batch\n",
      "Global Step: 47700 Epoch 21/50 Iteration: 47700 Avg. Training loss: 0.5561 0.0069 sec/batch\n",
      "Global Step: 47800 Epoch 21/50 Iteration: 47800 Avg. Training loss: 0.6310 0.0066 sec/batch\n",
      "Global Step: 47900 Epoch 21/50 Iteration: 47900 Avg. Training loss: 0.5818 0.0065 sec/batch\n",
      "Global Step: 48000 Epoch 21/50 Iteration: 48000 Avg. Training loss: 0.6332 0.0073 sec/batch\n",
      "Global Step: 48100 Epoch 21/50 Iteration: 48100 Avg. Training loss: 0.5909 0.0067 sec/batch\n",
      "Global Step: 48200 Epoch 21/50 Iteration: 48200 Avg. Training loss: 0.5262 0.0072 sec/batch\n",
      "Global Step: 48300 Epoch 21/50 Iteration: 48300 Avg. Training loss: 0.5685 0.0067 sec/batch\n",
      "Global Step: 48400 Epoch 21/50 Iteration: 48400 Avg. Training loss: 0.6104 0.0067 sec/batch\n",
      "Global Step: 48500 Epoch 21/50 Iteration: 48500 Avg. Training loss: 0.5075 0.0063 sec/batch\n",
      "Global Step: 48600 Epoch 21/50 Iteration: 48600 Avg. Training loss: 0.5510 0.0074 sec/batch\n",
      "Global Step: 48700 Epoch 21/50 Iteration: 48700 Avg. Training loss: 0.5814 0.0067 sec/batch\n",
      "Global Step: 48800 Epoch 21/50 Iteration: 48800 Avg. Training loss: 0.5134 0.0063 sec/batch\n",
      "Global Step: 48900 Epoch 21/50 Iteration: 48900 Avg. Training loss: 0.5718 0.0068 sec/batch\n",
      "Global Step: 49000 Epoch 21/50 Iteration: 49000 Avg. Training loss: 0.5246 0.0066 sec/batch\n",
      "Epoch 22/50 Threshold: 0.6549177815764775 Length of Training words: 2227878\n",
      "Global Step: 49100 Epoch 22/50 Iteration: 49100 Avg. Training loss: 0.6067 0.0037 sec/batch\n",
      "Global Step: 49200 Epoch 22/50 Iteration: 49200 Avg. Training loss: 0.6209 0.0089 sec/batch\n",
      "Global Step: 49300 Epoch 22/50 Iteration: 49300 Avg. Training loss: 0.5007 0.0090 sec/batch\n",
      "Global Step: 49400 Epoch 22/50 Iteration: 49400 Avg. Training loss: 0.6119 0.0067 sec/batch\n",
      "Global Step: 49500 Epoch 22/50 Iteration: 49500 Avg. Training loss: 0.5677 0.0073 sec/batch\n",
      "Global Step: 49600 Epoch 22/50 Iteration: 49600 Avg. Training loss: 0.4903 0.0096 sec/batch\n",
      "Global Step: 49700 Epoch 22/50 Iteration: 49700 Avg. Training loss: 0.5226 0.0075 sec/batch\n",
      "Global Step: 49800 Epoch 22/50 Iteration: 49800 Avg. Training loss: 0.5488 0.0066 sec/batch\n",
      "Global Step: 49900 Epoch 22/50 Iteration: 49900 Avg. Training loss: 0.5428 0.0068 sec/batch\n",
      "Global Step: 50000 Epoch 22/50 Iteration: 50000 Avg. Training loss: 0.6211 0.0067 sec/batch\n",
      "Global Step: 50100 Epoch 22/50 Iteration: 50100 Avg. Training loss: 0.5724 0.0064 sec/batch\n",
      "Global Step: 50200 Epoch 22/50 Iteration: 50200 Avg. Training loss: 0.6202 0.0066 sec/batch\n",
      "Global Step: 50300 Epoch 22/50 Iteration: 50300 Avg. Training loss: 0.5804 0.0076 sec/batch\n",
      "Global Step: 50400 Epoch 22/50 Iteration: 50400 Avg. Training loss: 0.5179 0.0065 sec/batch\n",
      "Global Step: 50500 Epoch 22/50 Iteration: 50500 Avg. Training loss: 0.5570 0.0064 sec/batch\n",
      "Global Step: 50600 Epoch 22/50 Iteration: 50600 Avg. Training loss: 0.6127 0.0068 sec/batch\n",
      "Global Step: 50700 Epoch 22/50 Iteration: 50700 Avg. Training loss: 0.4713 0.0066 sec/batch\n",
      "Global Step: 50800 Epoch 22/50 Iteration: 50800 Avg. Training loss: 0.5299 0.0089 sec/batch\n",
      "Global Step: 50900 Epoch 22/50 Iteration: 50900 Avg. Training loss: 0.6082 0.0090 sec/batch\n",
      "Global Step: 51000 Epoch 22/50 Iteration: 51000 Avg. Training loss: 0.5067 0.0097 sec/batch\n",
      "Global Step: 51100 Epoch 22/50 Iteration: 51100 Avg. Training loss: 0.5653 0.0069 sec/batch\n",
      "Global Step: 51200 Epoch 22/50 Iteration: 51200 Avg. Training loss: 0.5131 0.0080 sec/batch\n",
      "Epoch 23/50 Threshold: 0.6810977960933979 Length of Training words: 2268267\n",
      "Global Step: 51300 Epoch 23/50 Iteration: 51300 Avg. Training loss: 0.5751 0.0005 sec/batch\n",
      "Global Step: 51400 Epoch 23/50 Iteration: 51400 Avg. Training loss: 0.6304 0.0079 sec/batch\n",
      "Global Step: 51500 Epoch 23/50 Iteration: 51500 Avg. Training loss: 0.5160 0.0065 sec/batch\n",
      "Global Step: 51600 Epoch 23/50 Iteration: 51600 Avg. Training loss: 0.5899 0.0068 sec/batch\n",
      "Global Step: 51700 Epoch 23/50 Iteration: 51700 Avg. Training loss: 0.5387 0.0068 sec/batch\n",
      "Global Step: 51800 Epoch 23/50 Iteration: 51800 Avg. Training loss: 0.5126 0.0066 sec/batch\n",
      "Global Step: 51900 Epoch 23/50 Iteration: 51900 Avg. Training loss: 0.5215 0.0069 sec/batch\n",
      "Global Step: 52000 Epoch 23/50 Iteration: 52000 Avg. Training loss: 0.4701 0.0068 sec/batch\n",
      "Global Step: 52100 Epoch 23/50 Iteration: 52100 Avg. Training loss: 0.5964 0.0067 sec/batch\n",
      "Global Step: 52200 Epoch 23/50 Iteration: 52200 Avg. Training loss: 0.5798 0.0082 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 52300 Epoch 23/50 Iteration: 52300 Avg. Training loss: 0.6095 0.0067 sec/batch\n",
      "Global Step: 52400 Epoch 23/50 Iteration: 52400 Avg. Training loss: 0.5859 0.0082 sec/batch\n",
      "Global Step: 52500 Epoch 23/50 Iteration: 52500 Avg. Training loss: 0.5877 0.0073 sec/batch\n",
      "Global Step: 52600 Epoch 23/50 Iteration: 52600 Avg. Training loss: 0.5444 0.0078 sec/batch\n",
      "Global Step: 52700 Epoch 23/50 Iteration: 52700 Avg. Training loss: 0.5317 0.0081 sec/batch\n",
      "Global Step: 52800 Epoch 23/50 Iteration: 52800 Avg. Training loss: 0.5486 0.0102 sec/batch\n",
      "Global Step: 52900 Epoch 23/50 Iteration: 52900 Avg. Training loss: 0.5478 0.0103 sec/batch\n",
      "Global Step: 53000 Epoch 23/50 Iteration: 53000 Avg. Training loss: 0.5278 0.0095 sec/batch\n",
      "Global Step: 53100 Epoch 23/50 Iteration: 53100 Avg. Training loss: 0.5434 0.0076 sec/batch\n",
      "Global Step: 53200 Epoch 23/50 Iteration: 53200 Avg. Training loss: 0.5523 0.0078 sec/batch\n",
      "Global Step: 53300 Epoch 23/50 Iteration: 53300 Avg. Training loss: 0.4947 0.0090 sec/batch\n",
      "Global Step: 53400 Epoch 23/50 Iteration: 53400 Avg. Training loss: 0.5976 0.0070 sec/batch\n",
      "Global Step: 53500 Epoch 23/50 Iteration: 53500 Avg. Training loss: 0.4979 0.0078 sec/batch\n",
      "Epoch 24/50 Threshold: 0.7579822173753618 Length of Training words: 2381096\n",
      "Global Step: 53600 Epoch 24/50 Iteration: 53600 Avg. Training loss: 0.5902 0.0028 sec/batch\n",
      "Global Step: 53700 Epoch 24/50 Iteration: 53700 Avg. Training loss: 0.5995 0.0070 sec/batch\n",
      "Global Step: 53800 Epoch 24/50 Iteration: 53800 Avg. Training loss: 0.4861 0.0068 sec/batch\n",
      "Global Step: 53900 Epoch 24/50 Iteration: 53900 Avg. Training loss: 0.5978 0.0068 sec/batch\n",
      "Global Step: 54000 Epoch 24/50 Iteration: 54000 Avg. Training loss: 0.5072 0.0080 sec/batch\n",
      "Global Step: 54100 Epoch 24/50 Iteration: 54100 Avg. Training loss: 0.5042 0.0070 sec/batch\n",
      "Global Step: 54200 Epoch 24/50 Iteration: 54200 Avg. Training loss: 0.5037 0.0068 sec/batch\n",
      "Global Step: 54300 Epoch 24/50 Iteration: 54300 Avg. Training loss: 0.4684 0.0076 sec/batch\n",
      "Global Step: 54400 Epoch 24/50 Iteration: 54400 Avg. Training loss: 0.5760 0.0069 sec/batch\n",
      "Global Step: 54500 Epoch 24/50 Iteration: 54500 Avg. Training loss: 0.5383 0.0069 sec/batch\n",
      "Global Step: 54600 Epoch 24/50 Iteration: 54600 Avg. Training loss: 0.5949 0.0070 sec/batch\n",
      "Global Step: 54700 Epoch 24/50 Iteration: 54700 Avg. Training loss: 0.5886 0.0074 sec/batch\n",
      "Global Step: 54800 Epoch 24/50 Iteration: 54800 Avg. Training loss: 0.5447 0.0066 sec/batch\n",
      "Global Step: 54900 Epoch 24/50 Iteration: 54900 Avg. Training loss: 0.5733 0.0074 sec/batch\n",
      "Global Step: 55000 Epoch 24/50 Iteration: 55000 Avg. Training loss: 0.5008 0.0066 sec/batch\n",
      "Global Step: 55100 Epoch 24/50 Iteration: 55100 Avg. Training loss: 0.5491 0.0068 sec/batch\n",
      "Global Step: 55200 Epoch 24/50 Iteration: 55200 Avg. Training loss: 0.5854 0.0067 sec/batch\n",
      "Global Step: 55300 Epoch 24/50 Iteration: 55300 Avg. Training loss: 0.4512 0.0077 sec/batch\n",
      "Global Step: 55400 Epoch 24/50 Iteration: 55400 Avg. Training loss: 0.5148 0.0078 sec/batch\n",
      "Global Step: 55500 Epoch 24/50 Iteration: 55500 Avg. Training loss: 0.5909 0.0068 sec/batch\n",
      "Global Step: 55600 Epoch 24/50 Iteration: 55600 Avg. Training loss: 0.4794 0.0076 sec/batch\n",
      "Global Step: 55700 Epoch 24/50 Iteration: 55700 Avg. Training loss: 0.5314 0.0079 sec/batch\n",
      "Global Step: 55800 Epoch 24/50 Iteration: 55800 Avg. Training loss: 0.4903 0.0066 sec/batch\n",
      "Global Step: 55900 Epoch 24/50 Iteration: 55900 Avg. Training loss: 0.5431 0.0067 sec/batch\n",
      "Epoch 25/50 Threshold: 0.6310817292084921 Length of Training words: 2190865\n",
      "Global Step: 56000 Epoch 25/50 Iteration: 56000 Avg. Training loss: 0.6110 0.0038 sec/batch\n",
      "Global Step: 56100 Epoch 25/50 Iteration: 56100 Avg. Training loss: 0.5730 0.0082 sec/batch\n",
      "Global Step: 56200 Epoch 25/50 Iteration: 56200 Avg. Training loss: 0.5646 0.0070 sec/batch\n",
      "Global Step: 56300 Epoch 25/50 Iteration: 56300 Avg. Training loss: 0.5713 0.0070 sec/batch\n",
      "Global Step: 56400 Epoch 25/50 Iteration: 56400 Avg. Training loss: 0.5491 0.0064 sec/batch\n",
      "Global Step: 56500 Epoch 25/50 Iteration: 56500 Avg. Training loss: 0.5304 0.0068 sec/batch\n",
      "Global Step: 56600 Epoch 25/50 Iteration: 56600 Avg. Training loss: 0.5120 0.0062 sec/batch\n",
      "Global Step: 56700 Epoch 25/50 Iteration: 56700 Avg. Training loss: 0.5765 0.0066 sec/batch\n",
      "Global Step: 56800 Epoch 25/50 Iteration: 56800 Avg. Training loss: 0.5390 0.0069 sec/batch\n",
      "Global Step: 56900 Epoch 25/50 Iteration: 56900 Avg. Training loss: 0.6271 0.0068 sec/batch\n",
      "Global Step: 57000 Epoch 25/50 Iteration: 57000 Avg. Training loss: 0.6202 0.0066 sec/batch\n",
      "Global Step: 57100 Epoch 25/50 Iteration: 57100 Avg. Training loss: 0.5926 0.0072 sec/batch\n",
      "Global Step: 57200 Epoch 25/50 Iteration: 57200 Avg. Training loss: 0.5551 0.0084 sec/batch\n",
      "Global Step: 57300 Epoch 25/50 Iteration: 57300 Avg. Training loss: 0.5457 0.0079 sec/batch\n",
      "Global Step: 57400 Epoch 25/50 Iteration: 57400 Avg. Training loss: 0.5634 0.0082 sec/batch\n",
      "Global Step: 57500 Epoch 25/50 Iteration: 57500 Avg. Training loss: 0.5623 0.0096 sec/batch\n",
      "Global Step: 57600 Epoch 25/50 Iteration: 57600 Avg. Training loss: 0.5225 0.0069 sec/batch\n",
      "Global Step: 57700 Epoch 25/50 Iteration: 57700 Avg. Training loss: 0.5833 0.0071 sec/batch\n",
      "Global Step: 57800 Epoch 25/50 Iteration: 57800 Avg. Training loss: 0.5369 0.0073 sec/batch\n",
      "Global Step: 57900 Epoch 25/50 Iteration: 57900 Avg. Training loss: 0.5331 0.0072 sec/batch\n",
      "Global Step: 58000 Epoch 25/50 Iteration: 58000 Avg. Training loss: 0.5518 0.0075 sec/batch\n",
      "Global Step: 58100 Epoch 25/50 Iteration: 58100 Avg. Training loss: 0.5712 0.0073 sec/batch\n",
      "Epoch 26/50 Threshold: 0.8286024810170668 Length of Training words: 2481283\n",
      "Global Step: 58200 Epoch 26/50 Iteration: 58200 Avg. Training loss: 0.5847 0.0049 sec/batch\n",
      "Global Step: 58300 Epoch 26/50 Iteration: 58300 Avg. Training loss: 0.5550 0.0081 sec/batch\n",
      "Global Step: 58400 Epoch 26/50 Iteration: 58400 Avg. Training loss: 0.4808 0.0066 sec/batch\n",
      "Global Step: 58500 Epoch 26/50 Iteration: 58500 Avg. Training loss: 0.5651 0.0068 sec/batch\n",
      "Global Step: 58600 Epoch 26/50 Iteration: 58600 Avg. Training loss: 0.5145 0.0064 sec/batch\n",
      "Global Step: 58700 Epoch 26/50 Iteration: 58700 Avg. Training loss: 0.4872 0.0066 sec/batch\n",
      "Global Step: 58800 Epoch 26/50 Iteration: 58800 Avg. Training loss: 0.4826 0.0067 sec/batch\n",
      "Global Step: 58900 Epoch 26/50 Iteration: 58900 Avg. Training loss: 0.4652 0.0069 sec/batch\n",
      "Global Step: 59000 Epoch 26/50 Iteration: 59000 Avg. Training loss: 0.5615 0.0068 sec/batch\n",
      "Global Step: 59100 Epoch 26/50 Iteration: 59100 Avg. Training loss: 0.5089 0.0081 sec/batch\n",
      "Global Step: 59200 Epoch 26/50 Iteration: 59200 Avg. Training loss: 0.5605 0.0072 sec/batch\n",
      "Global Step: 59300 Epoch 26/50 Iteration: 59300 Avg. Training loss: 0.6011 0.0081 sec/batch\n",
      "Global Step: 59400 Epoch 26/50 Iteration: 59400 Avg. Training loss: 0.5660 0.0070 sec/batch\n",
      "Global Step: 59500 Epoch 26/50 Iteration: 59500 Avg. Training loss: 0.5500 0.0065 sec/batch\n",
      "Global Step: 59600 Epoch 26/50 Iteration: 59600 Avg. Training loss: 0.4937 0.0065 sec/batch\n",
      "Global Step: 59700 Epoch 26/50 Iteration: 59700 Avg. Training loss: 0.5326 0.0078 sec/batch\n",
      "Global Step: 59800 Epoch 26/50 Iteration: 59800 Avg. Training loss: 0.4999 0.0065 sec/batch\n",
      "Global Step: 59900 Epoch 26/50 Iteration: 59900 Avg. Training loss: 0.5274 0.0067 sec/batch\n",
      "Global Step: 60000 Epoch 26/50 Iteration: 60000 Avg. Training loss: 0.4853 0.0065 sec/batch\n",
      "Global Step: 60100 Epoch 26/50 Iteration: 60100 Avg. Training loss: 0.5208 0.0066 sec/batch\n",
      "Global Step: 60200 Epoch 26/50 Iteration: 60200 Avg. Training loss: 0.5379 0.0065 sec/batch\n",
      "Global Step: 60300 Epoch 26/50 Iteration: 60300 Avg. Training loss: 0.4659 0.0065 sec/batch\n",
      "Global Step: 60400 Epoch 26/50 Iteration: 60400 Avg. Training loss: 0.5485 0.0069 sec/batch\n",
      "Global Step: 60500 Epoch 26/50 Iteration: 60500 Avg. Training loss: 0.4873 0.0064 sec/batch\n",
      "Global Step: 60600 Epoch 26/50 Iteration: 60600 Avg. Training loss: 0.5309 0.0065 sec/batch\n",
      "Epoch 27/50 Threshold: 0.8347188630649091 Length of Training words: 2489888\n",
      "Global Step: 60700 Epoch 27/50 Iteration: 60700 Avg. Training loss: 0.5890 0.0058 sec/batch\n",
      "Global Step: 60800 Epoch 27/50 Iteration: 60800 Avg. Training loss: 0.5124 0.0067 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 60900 Epoch 27/50 Iteration: 60900 Avg. Training loss: 0.5239 0.0067 sec/batch\n",
      "Global Step: 61000 Epoch 27/50 Iteration: 61000 Avg. Training loss: 0.5468 0.0070 sec/batch\n",
      "Global Step: 61100 Epoch 27/50 Iteration: 61100 Avg. Training loss: 0.5307 0.0066 sec/batch\n",
      "Global Step: 61200 Epoch 27/50 Iteration: 61200 Avg. Training loss: 0.4378 0.0065 sec/batch\n",
      "Global Step: 61300 Epoch 27/50 Iteration: 61300 Avg. Training loss: 0.5010 0.0064 sec/batch\n",
      "Global Step: 61400 Epoch 27/50 Iteration: 61400 Avg. Training loss: 0.4677 0.0066 sec/batch\n",
      "Global Step: 61500 Epoch 27/50 Iteration: 61500 Avg. Training loss: 0.5583 0.0068 sec/batch\n",
      "Global Step: 61600 Epoch 27/50 Iteration: 61600 Avg. Training loss: 0.5354 0.0067 sec/batch\n",
      "Global Step: 61700 Epoch 27/50 Iteration: 61700 Avg. Training loss: 0.5740 0.0067 sec/batch\n",
      "Global Step: 61800 Epoch 27/50 Iteration: 61800 Avg. Training loss: 0.5769 0.0066 sec/batch\n",
      "Global Step: 61900 Epoch 27/50 Iteration: 61900 Avg. Training loss: 0.5419 0.0073 sec/batch\n",
      "Global Step: 62000 Epoch 27/50 Iteration: 62000 Avg. Training loss: 0.5575 0.0089 sec/batch\n",
      "Global Step: 62100 Epoch 27/50 Iteration: 62100 Avg. Training loss: 0.4865 0.0081 sec/batch\n",
      "Global Step: 62200 Epoch 27/50 Iteration: 62200 Avg. Training loss: 0.5230 0.0084 sec/batch\n",
      "Global Step: 62300 Epoch 27/50 Iteration: 62300 Avg. Training loss: 0.5268 0.0062 sec/batch\n",
      "Global Step: 62400 Epoch 27/50 Iteration: 62400 Avg. Training loss: 0.5192 0.0070 sec/batch\n",
      "Global Step: 62500 Epoch 27/50 Iteration: 62500 Avg. Training loss: 0.4720 0.0068 sec/batch\n",
      "Global Step: 62600 Epoch 27/50 Iteration: 62600 Avg. Training loss: 0.5283 0.0075 sec/batch\n",
      "Global Step: 62700 Epoch 27/50 Iteration: 62700 Avg. Training loss: 0.5212 0.0084 sec/batch\n",
      "Global Step: 62800 Epoch 27/50 Iteration: 62800 Avg. Training loss: 0.4800 0.0069 sec/batch\n",
      "Global Step: 62900 Epoch 27/50 Iteration: 62900 Avg. Training loss: 0.5346 0.0072 sec/batch\n",
      "Global Step: 63000 Epoch 27/50 Iteration: 63000 Avg. Training loss: 0.4814 0.0065 sec/batch\n",
      "Global Step: 63100 Epoch 27/50 Iteration: 63100 Avg. Training loss: 0.5379 0.0070 sec/batch\n",
      "Epoch 28/50 Threshold: 0.7087310044090905 Length of Training words: 2310554\n",
      "Global Step: 63200 Epoch 28/50 Iteration: 63200 Avg. Training loss: 0.6399 0.0081 sec/batch\n",
      "Global Step: 63300 Epoch 28/50 Iteration: 63300 Avg. Training loss: 0.5091 0.0069 sec/batch\n",
      "Global Step: 63400 Epoch 28/50 Iteration: 63400 Avg. Training loss: 0.5649 0.0081 sec/batch\n",
      "Global Step: 63500 Epoch 28/50 Iteration: 63500 Avg. Training loss: 0.5613 0.0067 sec/batch\n",
      "Global Step: 63600 Epoch 28/50 Iteration: 63600 Avg. Training loss: 0.5202 0.0067 sec/batch\n",
      "Global Step: 63700 Epoch 28/50 Iteration: 63700 Avg. Training loss: 0.5171 0.0067 sec/batch\n",
      "Global Step: 63800 Epoch 28/50 Iteration: 63800 Avg. Training loss: 0.4809 0.0081 sec/batch\n",
      "Global Step: 63900 Epoch 28/50 Iteration: 63900 Avg. Training loss: 0.5696 0.0067 sec/batch\n",
      "Global Step: 64000 Epoch 28/50 Iteration: 64000 Avg. Training loss: 0.5087 0.0069 sec/batch\n",
      "Global Step: 64100 Epoch 28/50 Iteration: 64100 Avg. Training loss: 0.5985 0.0082 sec/batch\n",
      "Global Step: 64200 Epoch 28/50 Iteration: 64200 Avg. Training loss: 0.6167 0.0105 sec/batch\n",
      "Global Step: 64300 Epoch 28/50 Iteration: 64300 Avg. Training loss: 0.5632 0.0065 sec/batch\n",
      "Global Step: 64400 Epoch 28/50 Iteration: 64400 Avg. Training loss: 0.5723 0.0066 sec/batch\n",
      "Global Step: 64500 Epoch 28/50 Iteration: 64500 Avg. Training loss: 0.5140 0.0071 sec/batch\n",
      "Global Step: 64600 Epoch 28/50 Iteration: 64600 Avg. Training loss: 0.5613 0.0078 sec/batch\n",
      "Global Step: 64700 Epoch 28/50 Iteration: 64700 Avg. Training loss: 0.5806 0.0075 sec/batch\n",
      "Global Step: 64800 Epoch 28/50 Iteration: 64800 Avg. Training loss: 0.4651 0.0063 sec/batch\n",
      "Global Step: 64900 Epoch 28/50 Iteration: 64900 Avg. Training loss: 0.5136 0.0065 sec/batch\n",
      "Global Step: 65000 Epoch 28/50 Iteration: 65000 Avg. Training loss: 0.5953 0.0082 sec/batch\n",
      "Global Step: 65100 Epoch 28/50 Iteration: 65100 Avg. Training loss: 0.4836 0.0081 sec/batch\n",
      "Global Step: 65200 Epoch 28/50 Iteration: 65200 Avg. Training loss: 0.5825 0.0082 sec/batch\n",
      "Global Step: 65300 Epoch 28/50 Iteration: 65300 Avg. Training loss: 0.4923 0.0097 sec/batch\n",
      "Global Step: 65400 Epoch 28/50 Iteration: 65400 Avg. Training loss: 0.5613 0.0093 sec/batch\n",
      "Epoch 29/50 Threshold: 0.6699144841981698 Length of Training words: 2249452\n",
      "Global Step: 65500 Epoch 29/50 Iteration: 65500 Avg. Training loss: 0.6312 0.0079 sec/batch\n",
      "Global Step: 65600 Epoch 29/50 Iteration: 65600 Avg. Training loss: 0.5281 0.0067 sec/batch\n",
      "Global Step: 65700 Epoch 29/50 Iteration: 65700 Avg. Training loss: 0.5621 0.0067 sec/batch\n",
      "Global Step: 65800 Epoch 29/50 Iteration: 65800 Avg. Training loss: 0.5760 0.0084 sec/batch\n",
      "Global Step: 65900 Epoch 29/50 Iteration: 65900 Avg. Training loss: 0.5293 0.0070 sec/batch\n",
      "Global Step: 66000 Epoch 29/50 Iteration: 66000 Avg. Training loss: 0.5118 0.0068 sec/batch\n",
      "Global Step: 66100 Epoch 29/50 Iteration: 66100 Avg. Training loss: 0.5034 0.0064 sec/batch\n",
      "Global Step: 66200 Epoch 29/50 Iteration: 66200 Avg. Training loss: 0.5618 0.0066 sec/batch\n",
      "Global Step: 66300 Epoch 29/50 Iteration: 66300 Avg. Training loss: 0.5536 0.0068 sec/batch\n",
      "Global Step: 66400 Epoch 29/50 Iteration: 66400 Avg. Training loss: 0.6214 0.0069 sec/batch\n",
      "Global Step: 66500 Epoch 29/50 Iteration: 66500 Avg. Training loss: 0.5984 0.0063 sec/batch\n",
      "Global Step: 66600 Epoch 29/50 Iteration: 66600 Avg. Training loss: 0.5820 0.0076 sec/batch\n",
      "Global Step: 66700 Epoch 29/50 Iteration: 66700 Avg. Training loss: 0.5439 0.0073 sec/batch\n",
      "Global Step: 66800 Epoch 29/50 Iteration: 66800 Avg. Training loss: 0.5481 0.0072 sec/batch\n",
      "Global Step: 66900 Epoch 29/50 Iteration: 66900 Avg. Training loss: 0.5393 0.0070 sec/batch\n",
      "Global Step: 67000 Epoch 29/50 Iteration: 67000 Avg. Training loss: 0.5751 0.0076 sec/batch\n",
      "Global Step: 67100 Epoch 29/50 Iteration: 67100 Avg. Training loss: 0.5182 0.0068 sec/batch\n",
      "Global Step: 67200 Epoch 29/50 Iteration: 67200 Avg. Training loss: 0.5421 0.0065 sec/batch\n",
      "Global Step: 67300 Epoch 29/50 Iteration: 67300 Avg. Training loss: 0.5540 0.0070 sec/batch\n",
      "Global Step: 67400 Epoch 29/50 Iteration: 67400 Avg. Training loss: 0.4960 0.0068 sec/batch\n",
      "Global Step: 67500 Epoch 29/50 Iteration: 67500 Avg. Training loss: 0.6027 0.0066 sec/batch\n",
      "Global Step: 67600 Epoch 29/50 Iteration: 67600 Avg. Training loss: 0.4994 0.0080 sec/batch\n",
      "Epoch 30/50 Threshold: 0.6882146281420994 Length of Training words: 2279075\n",
      "Global Step: 67700 Epoch 30/50 Iteration: 67700 Avg. Training loss: 0.6003 0.0038 sec/batch\n",
      "Global Step: 67800 Epoch 30/50 Iteration: 67800 Avg. Training loss: 0.6036 0.0131 sec/batch\n",
      "Global Step: 67900 Epoch 30/50 Iteration: 67900 Avg. Training loss: 0.4973 0.0107 sec/batch\n",
      "Global Step: 68000 Epoch 30/50 Iteration: 68000 Avg. Training loss: 0.5987 0.0104 sec/batch\n",
      "Global Step: 68100 Epoch 30/50 Iteration: 68100 Avg. Training loss: 0.5593 0.0072 sec/batch\n",
      "Global Step: 68200 Epoch 30/50 Iteration: 68200 Avg. Training loss: 0.4746 0.0076 sec/batch\n",
      "Global Step: 68300 Epoch 30/50 Iteration: 68300 Avg. Training loss: 0.5208 0.0082 sec/batch\n",
      "Global Step: 68400 Epoch 30/50 Iteration: 68400 Avg. Training loss: 0.5474 0.0074 sec/batch\n",
      "Global Step: 68500 Epoch 30/50 Iteration: 68500 Avg. Training loss: 0.5297 0.0085 sec/batch\n",
      "Global Step: 68600 Epoch 30/50 Iteration: 68600 Avg. Training loss: 0.6098 0.0063 sec/batch\n",
      "Global Step: 68700 Epoch 30/50 Iteration: 68700 Avg. Training loss: 0.5756 0.0072 sec/batch\n",
      "Global Step: 68800 Epoch 30/50 Iteration: 68800 Avg. Training loss: 0.5989 0.0069 sec/batch\n",
      "Global Step: 68900 Epoch 30/50 Iteration: 68900 Avg. Training loss: 0.5908 0.0065 sec/batch\n",
      "Global Step: 69000 Epoch 30/50 Iteration: 69000 Avg. Training loss: 0.5328 0.0068 sec/batch\n",
      "Global Step: 69100 Epoch 30/50 Iteration: 69100 Avg. Training loss: 0.5513 0.0069 sec/batch\n",
      "Global Step: 69200 Epoch 30/50 Iteration: 69200 Avg. Training loss: 0.5288 0.0064 sec/batch\n",
      "Global Step: 69300 Epoch 30/50 Iteration: 69300 Avg. Training loss: 0.5603 0.0068 sec/batch\n",
      "Global Step: 69400 Epoch 30/50 Iteration: 69400 Avg. Training loss: 0.4987 0.0068 sec/batch\n",
      "Global Step: 69500 Epoch 30/50 Iteration: 69500 Avg. Training loss: 0.5869 0.0068 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 69600 Epoch 30/50 Iteration: 69600 Avg. Training loss: 0.4979 0.0066 sec/batch\n",
      "Global Step: 69700 Epoch 30/50 Iteration: 69700 Avg. Training loss: 0.5317 0.0069 sec/batch\n",
      "Global Step: 69800 Epoch 30/50 Iteration: 69800 Avg. Training loss: 0.5383 0.0063 sec/batch\n",
      "Global Step: 69900 Epoch 30/50 Iteration: 69900 Avg. Training loss: 0.5437 0.0064 sec/batch\n",
      "Epoch 31/50 Threshold: 0.8607716407876104 Length of Training words: 2525518\n",
      "Global Step: 70000 Epoch 31/50 Iteration: 70000 Avg. Training loss: 0.5933 0.0038 sec/batch\n",
      "Global Step: 70100 Epoch 31/50 Iteration: 70100 Avg. Training loss: 0.5448 0.0066 sec/batch\n",
      "Global Step: 70200 Epoch 31/50 Iteration: 70200 Avg. Training loss: 0.4710 0.0062 sec/batch\n",
      "Global Step: 70300 Epoch 31/50 Iteration: 70300 Avg. Training loss: 0.5864 0.0066 sec/batch\n",
      "Global Step: 70400 Epoch 31/50 Iteration: 70400 Avg. Training loss: 0.4822 0.0062 sec/batch\n",
      "Global Step: 70500 Epoch 31/50 Iteration: 70500 Avg. Training loss: 0.4912 0.0067 sec/batch\n",
      "Global Step: 70600 Epoch 31/50 Iteration: 70600 Avg. Training loss: 0.4968 0.0067 sec/batch\n",
      "Global Step: 70700 Epoch 31/50 Iteration: 70700 Avg. Training loss: 0.4677 0.0063 sec/batch\n",
      "Global Step: 70800 Epoch 31/50 Iteration: 70800 Avg. Training loss: 0.5332 0.0069 sec/batch\n",
      "Global Step: 70900 Epoch 31/50 Iteration: 70900 Avg. Training loss: 0.4806 0.0067 sec/batch\n",
      "Global Step: 71000 Epoch 31/50 Iteration: 71000 Avg. Training loss: 0.6005 0.0065 sec/batch\n",
      "Global Step: 71100 Epoch 31/50 Iteration: 71100 Avg. Training loss: 0.5220 0.0065 sec/batch\n",
      "Global Step: 71200 Epoch 31/50 Iteration: 71200 Avg. Training loss: 0.5769 0.0067 sec/batch\n",
      "Global Step: 71300 Epoch 31/50 Iteration: 71300 Avg. Training loss: 0.5506 0.0065 sec/batch\n",
      "Global Step: 71400 Epoch 31/50 Iteration: 71400 Avg. Training loss: 0.5191 0.0066 sec/batch\n",
      "Global Step: 71500 Epoch 31/50 Iteration: 71500 Avg. Training loss: 0.4944 0.0063 sec/batch\n",
      "Global Step: 71600 Epoch 31/50 Iteration: 71600 Avg. Training loss: 0.5073 0.0064 sec/batch\n",
      "Global Step: 71700 Epoch 31/50 Iteration: 71700 Avg. Training loss: 0.5768 0.0067 sec/batch\n",
      "Global Step: 71800 Epoch 31/50 Iteration: 71800 Avg. Training loss: 0.4448 0.0067 sec/batch\n",
      "Global Step: 71900 Epoch 31/50 Iteration: 71900 Avg. Training loss: 0.4717 0.0065 sec/batch\n",
      "Global Step: 72000 Epoch 31/50 Iteration: 72000 Avg. Training loss: 0.5840 0.0065 sec/batch\n",
      "Global Step: 72100 Epoch 31/50 Iteration: 72100 Avg. Training loss: 0.4731 0.0066 sec/batch\n",
      "Global Step: 72200 Epoch 31/50 Iteration: 72200 Avg. Training loss: 0.4900 0.0064 sec/batch\n",
      "Global Step: 72300 Epoch 31/50 Iteration: 72300 Avg. Training loss: 0.5344 0.0069 sec/batch\n",
      "Global Step: 72400 Epoch 31/50 Iteration: 72400 Avg. Training loss: 0.4673 0.0065 sec/batch\n",
      "Epoch 32/50 Threshold: 0.7742228067186813 Length of Training words: 2404634\n",
      "Global Step: 72500 Epoch 32/50 Iteration: 72500 Avg. Training loss: 0.5639 0.0023 sec/batch\n",
      "Global Step: 72600 Epoch 32/50 Iteration: 72600 Avg. Training loss: 0.6002 0.0066 sec/batch\n",
      "Global Step: 72700 Epoch 32/50 Iteration: 72700 Avg. Training loss: 0.4981 0.0065 sec/batch\n",
      "Global Step: 72800 Epoch 32/50 Iteration: 72800 Avg. Training loss: 0.5824 0.0066 sec/batch\n",
      "Global Step: 72900 Epoch 32/50 Iteration: 72900 Avg. Training loss: 0.5042 0.0065 sec/batch\n",
      "Global Step: 73000 Epoch 32/50 Iteration: 73000 Avg. Training loss: 0.5041 0.0065 sec/batch\n",
      "Global Step: 73100 Epoch 32/50 Iteration: 73100 Avg. Training loss: 0.5090 0.0063 sec/batch\n",
      "Global Step: 73200 Epoch 32/50 Iteration: 73200 Avg. Training loss: 0.4835 0.0067 sec/batch\n",
      "Global Step: 73300 Epoch 32/50 Iteration: 73300 Avg. Training loss: 0.5501 0.0064 sec/batch\n",
      "Global Step: 73400 Epoch 32/50 Iteration: 73400 Avg. Training loss: 0.5058 0.0063 sec/batch\n",
      "Global Step: 73500 Epoch 32/50 Iteration: 73500 Avg. Training loss: 0.5775 0.0067 sec/batch\n",
      "Global Step: 73600 Epoch 32/50 Iteration: 73600 Avg. Training loss: 0.6095 0.0069 sec/batch\n",
      "Global Step: 73700 Epoch 32/50 Iteration: 73700 Avg. Training loss: 0.5585 0.0067 sec/batch\n",
      "Global Step: 73800 Epoch 32/50 Iteration: 73800 Avg. Training loss: 0.5705 0.0067 sec/batch\n",
      "Global Step: 73900 Epoch 32/50 Iteration: 73900 Avg. Training loss: 0.4889 0.0065 sec/batch\n",
      "Global Step: 74000 Epoch 32/50 Iteration: 74000 Avg. Training loss: 0.5357 0.0062 sec/batch\n",
      "Global Step: 74100 Epoch 32/50 Iteration: 74100 Avg. Training loss: 0.5510 0.0067 sec/batch\n",
      "Global Step: 74200 Epoch 32/50 Iteration: 74200 Avg. Training loss: 0.5209 0.0066 sec/batch\n",
      "Global Step: 74300 Epoch 32/50 Iteration: 74300 Avg. Training loss: 0.4851 0.0067 sec/batch\n",
      "Global Step: 74400 Epoch 32/50 Iteration: 74400 Avg. Training loss: 0.5608 0.0070 sec/batch\n",
      "Global Step: 74500 Epoch 32/50 Iteration: 74500 Avg. Training loss: 0.5037 0.0066 sec/batch\n",
      "Global Step: 74600 Epoch 32/50 Iteration: 74600 Avg. Training loss: 0.4845 0.0066 sec/batch\n",
      "Global Step: 74700 Epoch 32/50 Iteration: 74700 Avg. Training loss: 0.5714 0.0066 sec/batch\n",
      "Global Step: 74800 Epoch 32/50 Iteration: 74800 Avg. Training loss: 0.4732 0.0068 sec/batch\n",
      "Epoch 33/50 Threshold: 0.7262127535117937 Length of Training words: 2336025\n",
      "Global Step: 74900 Epoch 33/50 Iteration: 74900 Avg. Training loss: 0.5791 0.0023 sec/batch\n",
      "Global Step: 75000 Epoch 33/50 Iteration: 75000 Avg. Training loss: 0.6070 0.0069 sec/batch\n",
      "Global Step: 75100 Epoch 33/50 Iteration: 75100 Avg. Training loss: 0.5038 0.0068 sec/batch\n",
      "Global Step: 75200 Epoch 33/50 Iteration: 75200 Avg. Training loss: 0.5920 0.0075 sec/batch\n",
      "Global Step: 75300 Epoch 33/50 Iteration: 75300 Avg. Training loss: 0.5145 0.0097 sec/batch\n",
      "Global Step: 75400 Epoch 33/50 Iteration: 75400 Avg. Training loss: 0.5086 0.0106 sec/batch\n",
      "Global Step: 75500 Epoch 33/50 Iteration: 75500 Avg. Training loss: 0.4982 0.0068 sec/batch\n",
      "Global Step: 75600 Epoch 33/50 Iteration: 75600 Avg. Training loss: 0.4751 0.0066 sec/batch\n",
      "Global Step: 75700 Epoch 33/50 Iteration: 75700 Avg. Training loss: 0.5847 0.0069 sec/batch\n",
      "Global Step: 75800 Epoch 33/50 Iteration: 75800 Avg. Training loss: 0.5625 0.0103 sec/batch\n",
      "Global Step: 75900 Epoch 33/50 Iteration: 75900 Avg. Training loss: 0.6099 0.0081 sec/batch\n",
      "Global Step: 76000 Epoch 33/50 Iteration: 76000 Avg. Training loss: 0.5829 0.0079 sec/batch\n",
      "Global Step: 76100 Epoch 33/50 Iteration: 76100 Avg. Training loss: 0.5666 0.0072 sec/batch\n",
      "Global Step: 76200 Epoch 33/50 Iteration: 76200 Avg. Training loss: 0.5461 0.0085 sec/batch\n",
      "Global Step: 76300 Epoch 33/50 Iteration: 76300 Avg. Training loss: 0.5049 0.0083 sec/batch\n",
      "Global Step: 76400 Epoch 33/50 Iteration: 76400 Avg. Training loss: 0.5588 0.0087 sec/batch\n",
      "Global Step: 76500 Epoch 33/50 Iteration: 76500 Avg. Training loss: 0.5803 0.0065 sec/batch\n",
      "Global Step: 76600 Epoch 33/50 Iteration: 76600 Avg. Training loss: 0.4749 0.0068 sec/batch\n",
      "Global Step: 76700 Epoch 33/50 Iteration: 76700 Avg. Training loss: 0.5311 0.0084 sec/batch\n",
      "Global Step: 76800 Epoch 33/50 Iteration: 76800 Avg. Training loss: 0.5637 0.0066 sec/batch\n",
      "Global Step: 76900 Epoch 33/50 Iteration: 76900 Avg. Training loss: 0.4846 0.0064 sec/batch\n",
      "Global Step: 77000 Epoch 33/50 Iteration: 77000 Avg. Training loss: 0.5560 0.0069 sec/batch\n",
      "Global Step: 77100 Epoch 33/50 Iteration: 77100 Avg. Training loss: 0.5038 0.0078 sec/batch\n",
      "Global Step: 77200 Epoch 33/50 Iteration: 77200 Avg. Training loss: 0.5411 0.0080 sec/batch\n",
      "Epoch 34/50 Threshold: 0.8231308963953272 Length of Training words: 2473613\n",
      "Global Step: 77300 Epoch 34/50 Iteration: 77300 Avg. Training loss: 0.6192 0.0060 sec/batch\n",
      "Global Step: 77400 Epoch 34/50 Iteration: 77400 Avg. Training loss: 0.5077 0.0078 sec/batch\n",
      "Global Step: 77500 Epoch 34/50 Iteration: 77500 Avg. Training loss: 0.5266 0.0069 sec/batch\n",
      "Global Step: 77600 Epoch 34/50 Iteration: 77600 Avg. Training loss: 0.5348 0.0089 sec/batch\n",
      "Global Step: 77700 Epoch 34/50 Iteration: 77700 Avg. Training loss: 0.5367 0.0074 sec/batch\n",
      "Global Step: 77800 Epoch 34/50 Iteration: 77800 Avg. Training loss: 0.4604 0.0074 sec/batch\n",
      "Global Step: 77900 Epoch 34/50 Iteration: 77900 Avg. Training loss: 0.4823 0.0077 sec/batch\n",
      "Global Step: 78000 Epoch 34/50 Iteration: 78000 Avg. Training loss: 0.4995 0.0085 sec/batch\n",
      "Global Step: 78100 Epoch 34/50 Iteration: 78100 Avg. Training loss: 0.5198 0.0067 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 78200 Epoch 34/50 Iteration: 78200 Avg. Training loss: 0.5664 0.0092 sec/batch\n",
      "Global Step: 78300 Epoch 34/50 Iteration: 78300 Avg. Training loss: 0.5790 0.0066 sec/batch\n",
      "Global Step: 78400 Epoch 34/50 Iteration: 78400 Avg. Training loss: 0.5642 0.0067 sec/batch\n",
      "Global Step: 78500 Epoch 34/50 Iteration: 78500 Avg. Training loss: 0.5331 0.0062 sec/batch\n",
      "Global Step: 78600 Epoch 34/50 Iteration: 78600 Avg. Training loss: 0.5528 0.0086 sec/batch\n",
      "Global Step: 78700 Epoch 34/50 Iteration: 78700 Avg. Training loss: 0.4826 0.0067 sec/batch\n",
      "Global Step: 78800 Epoch 34/50 Iteration: 78800 Avg. Training loss: 0.5313 0.0062 sec/batch\n",
      "Global Step: 78900 Epoch 34/50 Iteration: 78900 Avg. Training loss: 0.5646 0.0068 sec/batch\n",
      "Global Step: 79000 Epoch 34/50 Iteration: 79000 Avg. Training loss: 0.4872 0.0067 sec/batch\n",
      "Global Step: 79100 Epoch 34/50 Iteration: 79100 Avg. Training loss: 0.4761 0.0064 sec/batch\n",
      "Global Step: 79200 Epoch 34/50 Iteration: 79200 Avg. Training loss: 0.5583 0.0064 sec/batch\n",
      "Global Step: 79300 Epoch 34/50 Iteration: 79300 Avg. Training loss: 0.4864 0.0065 sec/batch\n",
      "Global Step: 79400 Epoch 34/50 Iteration: 79400 Avg. Training loss: 0.4760 0.0065 sec/batch\n",
      "Global Step: 79500 Epoch 34/50 Iteration: 79500 Avg. Training loss: 0.5608 0.0065 sec/batch\n",
      "Global Step: 79600 Epoch 34/50 Iteration: 79600 Avg. Training loss: 0.4638 0.0066 sec/batch\n",
      "Epoch 35/50 Threshold: 0.7600729948990718 Length of Training words: 2385329\n",
      "Global Step: 79700 Epoch 35/50 Iteration: 79700 Avg. Training loss: 0.5734 0.0016 sec/batch\n",
      "Global Step: 79800 Epoch 35/50 Iteration: 79800 Avg. Training loss: 0.5978 0.0063 sec/batch\n",
      "Global Step: 79900 Epoch 35/50 Iteration: 79900 Avg. Training loss: 0.5052 0.0066 sec/batch\n",
      "Global Step: 80000 Epoch 35/50 Iteration: 80000 Avg. Training loss: 0.5734 0.0065 sec/batch\n",
      "Global Step: 80100 Epoch 35/50 Iteration: 80100 Avg. Training loss: 0.5356 0.0066 sec/batch\n",
      "Global Step: 80200 Epoch 35/50 Iteration: 80200 Avg. Training loss: 0.4910 0.0064 sec/batch\n",
      "Global Step: 80300 Epoch 35/50 Iteration: 80300 Avg. Training loss: 0.5135 0.0065 sec/batch\n",
      "Global Step: 80400 Epoch 35/50 Iteration: 80400 Avg. Training loss: 0.4739 0.0072 sec/batch\n",
      "Global Step: 80500 Epoch 35/50 Iteration: 80500 Avg. Training loss: 0.5647 0.0068 sec/batch\n",
      "Global Step: 80600 Epoch 35/50 Iteration: 80600 Avg. Training loss: 0.4953 0.0062 sec/batch\n",
      "Global Step: 80700 Epoch 35/50 Iteration: 80700 Avg. Training loss: 0.5925 0.0077 sec/batch\n",
      "Global Step: 80800 Epoch 35/50 Iteration: 80800 Avg. Training loss: 0.5971 0.0074 sec/batch\n",
      "Global Step: 80900 Epoch 35/50 Iteration: 80900 Avg. Training loss: 0.5723 0.0093 sec/batch\n",
      "Global Step: 81000 Epoch 35/50 Iteration: 81000 Avg. Training loss: 0.5655 0.0067 sec/batch\n",
      "Global Step: 81100 Epoch 35/50 Iteration: 81100 Avg. Training loss: 0.4878 0.0063 sec/batch\n",
      "Global Step: 81200 Epoch 35/50 Iteration: 81200 Avg. Training loss: 0.5406 0.0069 sec/batch\n",
      "Global Step: 81300 Epoch 35/50 Iteration: 81300 Avg. Training loss: 0.5536 0.0070 sec/batch\n",
      "Global Step: 81400 Epoch 35/50 Iteration: 81400 Avg. Training loss: 0.5213 0.0066 sec/batch\n",
      "Global Step: 81500 Epoch 35/50 Iteration: 81500 Avg. Training loss: 0.4893 0.0065 sec/batch\n",
      "Global Step: 81600 Epoch 35/50 Iteration: 81600 Avg. Training loss: 0.5668 0.0065 sec/batch\n",
      "Global Step: 81700 Epoch 35/50 Iteration: 81700 Avg. Training loss: 0.4946 0.0065 sec/batch\n",
      "Global Step: 81800 Epoch 35/50 Iteration: 81800 Avg. Training loss: 0.4881 0.0066 sec/batch\n",
      "Global Step: 81900 Epoch 35/50 Iteration: 81900 Avg. Training loss: 0.5768 0.0065 sec/batch\n",
      "Global Step: 82000 Epoch 35/50 Iteration: 82000 Avg. Training loss: 0.4847 0.0069 sec/batch\n",
      "Epoch 36/50 Threshold: 0.7333294113571045 Length of Training words: 2345902\n",
      "Global Step: 82100 Epoch 36/50 Iteration: 82100 Avg. Training loss: 0.5819 0.0024 sec/batch\n",
      "Global Step: 82200 Epoch 36/50 Iteration: 82200 Avg. Training loss: 0.6028 0.0065 sec/batch\n",
      "Global Step: 82300 Epoch 36/50 Iteration: 82300 Avg. Training loss: 0.4883 0.0063 sec/batch\n",
      "Global Step: 82400 Epoch 36/50 Iteration: 82400 Avg. Training loss: 0.6044 0.0065 sec/batch\n",
      "Global Step: 82500 Epoch 36/50 Iteration: 82500 Avg. Training loss: 0.5186 0.0067 sec/batch\n",
      "Global Step: 82600 Epoch 36/50 Iteration: 82600 Avg. Training loss: 0.5006 0.0065 sec/batch\n",
      "Global Step: 82700 Epoch 36/50 Iteration: 82700 Avg. Training loss: 0.4940 0.0064 sec/batch\n",
      "Global Step: 82800 Epoch 36/50 Iteration: 82800 Avg. Training loss: 0.4773 0.0067 sec/batch\n",
      "Global Step: 82900 Epoch 36/50 Iteration: 82900 Avg. Training loss: 0.5853 0.0063 sec/batch\n",
      "Global Step: 83000 Epoch 36/50 Iteration: 83000 Avg. Training loss: 0.5660 0.0066 sec/batch\n",
      "Global Step: 83100 Epoch 36/50 Iteration: 83100 Avg. Training loss: 0.6070 0.0067 sec/batch\n",
      "Global Step: 83200 Epoch 36/50 Iteration: 83200 Avg. Training loss: 0.5740 0.0066 sec/batch\n",
      "Global Step: 83300 Epoch 36/50 Iteration: 83300 Avg. Training loss: 0.5689 0.0063 sec/batch\n",
      "Global Step: 83400 Epoch 36/50 Iteration: 83400 Avg. Training loss: 0.5403 0.0063 sec/batch\n",
      "Global Step: 83500 Epoch 36/50 Iteration: 83500 Avg. Training loss: 0.5053 0.0067 sec/batch\n",
      "Global Step: 83600 Epoch 36/50 Iteration: 83600 Avg. Training loss: 0.5588 0.0068 sec/batch\n",
      "Global Step: 83700 Epoch 36/50 Iteration: 83700 Avg. Training loss: 0.5782 0.0062 sec/batch\n",
      "Global Step: 83800 Epoch 36/50 Iteration: 83800 Avg. Training loss: 0.4748 0.0067 sec/batch\n",
      "Global Step: 83900 Epoch 36/50 Iteration: 83900 Avg. Training loss: 0.5250 0.0065 sec/batch\n",
      "Global Step: 84000 Epoch 36/50 Iteration: 84000 Avg. Training loss: 0.5650 0.0063 sec/batch\n",
      "Global Step: 84100 Epoch 36/50 Iteration: 84100 Avg. Training loss: 0.4936 0.0068 sec/batch\n",
      "Global Step: 84200 Epoch 36/50 Iteration: 84200 Avg. Training loss: 0.5568 0.0068 sec/batch\n",
      "Global Step: 84300 Epoch 36/50 Iteration: 84300 Avg. Training loss: 0.4953 0.0066 sec/batch\n",
      "Global Step: 84400 Epoch 36/50 Iteration: 84400 Avg. Training loss: 0.5440 0.0066 sec/batch\n",
      "Epoch 37/50 Threshold: 0.6026413409947156 Length of Training words: 2145832\n",
      "Global Step: 84500 Epoch 37/50 Iteration: 84500 Avg. Training loss: 0.6555 0.0063 sec/batch\n",
      "Global Step: 84600 Epoch 37/50 Iteration: 84600 Avg. Training loss: 0.5287 0.0065 sec/batch\n",
      "Global Step: 84700 Epoch 37/50 Iteration: 84700 Avg. Training loss: 0.6088 0.0068 sec/batch\n",
      "Global Step: 84800 Epoch 37/50 Iteration: 84800 Avg. Training loss: 0.5399 0.0067 sec/batch\n",
      "Global Step: 84900 Epoch 37/50 Iteration: 84900 Avg. Training loss: 0.5400 0.0067 sec/batch\n",
      "Global Step: 85000 Epoch 37/50 Iteration: 85000 Avg. Training loss: 0.5236 0.0063 sec/batch\n",
      "Global Step: 85100 Epoch 37/50 Iteration: 85100 Avg. Training loss: 0.5498 0.0067 sec/batch\n",
      "Global Step: 85200 Epoch 37/50 Iteration: 85200 Avg. Training loss: 0.5612 0.0066 sec/batch\n",
      "Global Step: 85300 Epoch 37/50 Iteration: 85300 Avg. Training loss: 0.6354 0.0065 sec/batch\n",
      "Global Step: 85400 Epoch 37/50 Iteration: 85400 Avg. Training loss: 0.5861 0.0068 sec/batch\n",
      "Global Step: 85500 Epoch 37/50 Iteration: 85500 Avg. Training loss: 0.6316 0.0068 sec/batch\n",
      "Global Step: 85600 Epoch 37/50 Iteration: 85600 Avg. Training loss: 0.5904 0.0066 sec/batch\n",
      "Global Step: 85700 Epoch 37/50 Iteration: 85700 Avg. Training loss: 0.5242 0.0069 sec/batch\n",
      "Global Step: 85800 Epoch 37/50 Iteration: 85800 Avg. Training loss: 0.5872 0.0068 sec/batch\n",
      "Global Step: 85900 Epoch 37/50 Iteration: 85900 Avg. Training loss: 0.6079 0.0066 sec/batch\n",
      "Global Step: 86000 Epoch 37/50 Iteration: 86000 Avg. Training loss: 0.5022 0.0065 sec/batch\n",
      "Global Step: 86100 Epoch 37/50 Iteration: 86100 Avg. Training loss: 0.5559 0.0065 sec/batch\n",
      "Global Step: 86200 Epoch 37/50 Iteration: 86200 Avg. Training loss: 0.5730 0.0062 sec/batch\n",
      "Global Step: 86300 Epoch 37/50 Iteration: 86300 Avg. Training loss: 0.5017 0.0070 sec/batch\n",
      "Global Step: 86400 Epoch 37/50 Iteration: 86400 Avg. Training loss: 0.6189 0.0069 sec/batch\n",
      "Global Step: 86500 Epoch 37/50 Iteration: 86500 Avg. Training loss: 0.5164 0.0061 sec/batch\n",
      "Epoch 38/50 Threshold: 0.8211746168851805 Length of Training words: 2471065\n",
      "Global Step: 86600 Epoch 38/50 Iteration: 86600 Avg. Training loss: 0.6000 0.0029 sec/batch\n",
      "Global Step: 86700 Epoch 38/50 Iteration: 86700 Avg. Training loss: 0.5807 0.0065 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 86800 Epoch 38/50 Iteration: 86800 Avg. Training loss: 0.4793 0.0065 sec/batch\n",
      "Global Step: 86900 Epoch 38/50 Iteration: 86900 Avg. Training loss: 0.5780 0.0065 sec/batch\n",
      "Global Step: 87000 Epoch 38/50 Iteration: 87000 Avg. Training loss: 0.4994 0.0067 sec/batch\n",
      "Global Step: 87100 Epoch 38/50 Iteration: 87100 Avg. Training loss: 0.4966 0.0062 sec/batch\n",
      "Global Step: 87200 Epoch 38/50 Iteration: 87200 Avg. Training loss: 0.5011 0.0067 sec/batch\n",
      "Global Step: 87300 Epoch 38/50 Iteration: 87300 Avg. Training loss: 0.4717 0.0068 sec/batch\n",
      "Global Step: 87400 Epoch 38/50 Iteration: 87400 Avg. Training loss: 0.5492 0.0069 sec/batch\n",
      "Global Step: 87500 Epoch 38/50 Iteration: 87500 Avg. Training loss: 0.4775 0.0078 sec/batch\n",
      "Global Step: 87600 Epoch 38/50 Iteration: 87600 Avg. Training loss: 0.6061 0.0088 sec/batch\n",
      "Global Step: 87700 Epoch 38/50 Iteration: 87700 Avg. Training loss: 0.5500 0.0078 sec/batch\n",
      "Global Step: 87800 Epoch 38/50 Iteration: 87800 Avg. Training loss: 0.5863 0.0066 sec/batch\n",
      "Global Step: 87900 Epoch 38/50 Iteration: 87900 Avg. Training loss: 0.5566 0.0080 sec/batch\n",
      "Global Step: 88000 Epoch 38/50 Iteration: 88000 Avg. Training loss: 0.5134 0.0119 sec/batch\n",
      "Global Step: 88100 Epoch 38/50 Iteration: 88100 Avg. Training loss: 0.5104 0.0070 sec/batch\n",
      "Global Step: 88200 Epoch 38/50 Iteration: 88200 Avg. Training loss: 0.5000 0.0066 sec/batch\n",
      "Global Step: 88300 Epoch 38/50 Iteration: 88300 Avg. Training loss: 0.5272 0.0079 sec/batch\n",
      "Global Step: 88400 Epoch 38/50 Iteration: 88400 Avg. Training loss: 0.5161 0.0065 sec/batch\n",
      "Global Step: 88500 Epoch 38/50 Iteration: 88500 Avg. Training loss: 0.4969 0.0066 sec/batch\n",
      "Global Step: 88600 Epoch 38/50 Iteration: 88600 Avg. Training loss: 0.5473 0.0071 sec/batch\n",
      "Global Step: 88700 Epoch 38/50 Iteration: 88700 Avg. Training loss: 0.4716 0.0076 sec/batch\n",
      "Global Step: 88800 Epoch 38/50 Iteration: 88800 Avg. Training loss: 0.5410 0.0078 sec/batch\n",
      "Global Step: 88900 Epoch 38/50 Iteration: 88900 Avg. Training loss: 0.4699 0.0064 sec/batch\n",
      "Global Step: 89000 Epoch 38/50 Iteration: 89000 Avg. Training loss: 0.5487 0.0073 sec/batch\n",
      "Epoch 39/50 Threshold: 0.6840613260474472 Length of Training words: 2272564\n",
      "Global Step: 89100 Epoch 39/50 Iteration: 89100 Avg. Training loss: 0.6004 0.0052 sec/batch\n",
      "Global Step: 89200 Epoch 39/50 Iteration: 89200 Avg. Training loss: 0.5531 0.0068 sec/batch\n",
      "Global Step: 89300 Epoch 39/50 Iteration: 89300 Avg. Training loss: 0.5458 0.0075 sec/batch\n",
      "Global Step: 89400 Epoch 39/50 Iteration: 89400 Avg. Training loss: 0.5657 0.0085 sec/batch\n",
      "Global Step: 89500 Epoch 39/50 Iteration: 89500 Avg. Training loss: 0.5284 0.0079 sec/batch\n",
      "Global Step: 89600 Epoch 39/50 Iteration: 89600 Avg. Training loss: 0.5207 0.0069 sec/batch\n",
      "Global Step: 89700 Epoch 39/50 Iteration: 89700 Avg. Training loss: 0.5156 0.0097 sec/batch\n",
      "Global Step: 89800 Epoch 39/50 Iteration: 89800 Avg. Training loss: 0.5493 0.0074 sec/batch\n",
      "Global Step: 89900 Epoch 39/50 Iteration: 89900 Avg. Training loss: 0.5208 0.0068 sec/batch\n",
      "Global Step: 90000 Epoch 39/50 Iteration: 90000 Avg. Training loss: 0.5976 0.0073 sec/batch\n",
      "Global Step: 90100 Epoch 39/50 Iteration: 90100 Avg. Training loss: 0.6259 0.0068 sec/batch\n",
      "Global Step: 90200 Epoch 39/50 Iteration: 90200 Avg. Training loss: 0.5692 0.0067 sec/batch\n",
      "Global Step: 90300 Epoch 39/50 Iteration: 90300 Avg. Training loss: 0.5767 0.0066 sec/batch\n",
      "Global Step: 90400 Epoch 39/50 Iteration: 90400 Avg. Training loss: 0.5189 0.0069 sec/batch\n",
      "Global Step: 90500 Epoch 39/50 Iteration: 90500 Avg. Training loss: 0.5653 0.0099 sec/batch\n",
      "Global Step: 90600 Epoch 39/50 Iteration: 90600 Avg. Training loss: 0.5892 0.0094 sec/batch\n",
      "Global Step: 90700 Epoch 39/50 Iteration: 90700 Avg. Training loss: 0.4831 0.0080 sec/batch\n",
      "Global Step: 90800 Epoch 39/50 Iteration: 90800 Avg. Training loss: 0.5200 0.0093 sec/batch\n",
      "Global Step: 90900 Epoch 39/50 Iteration: 90900 Avg. Training loss: 0.5868 0.0075 sec/batch\n",
      "Global Step: 91000 Epoch 39/50 Iteration: 91000 Avg. Training loss: 0.4970 0.0087 sec/batch\n",
      "Global Step: 91100 Epoch 39/50 Iteration: 91100 Avg. Training loss: 0.5627 0.0066 sec/batch\n",
      "Global Step: 91200 Epoch 39/50 Iteration: 91200 Avg. Training loss: 0.5066 0.0072 sec/batch\n",
      "Epoch 40/50 Threshold: 0.8770383119282995 Length of Training words: 2547470\n",
      "Global Step: 91300 Epoch 40/50 Iteration: 91300 Avg. Training loss: 0.5674 0.0002 sec/batch\n",
      "Global Step: 91400 Epoch 40/50 Iteration: 91400 Avg. Training loss: 0.6103 0.0064 sec/batch\n",
      "Global Step: 91500 Epoch 40/50 Iteration: 91500 Avg. Training loss: 0.4881 0.0075 sec/batch\n",
      "Global Step: 91600 Epoch 40/50 Iteration: 91600 Avg. Training loss: 0.5180 0.0086 sec/batch\n",
      "Global Step: 91700 Epoch 40/50 Iteration: 91700 Avg. Training loss: 0.5190 0.0064 sec/batch\n",
      "Global Step: 91800 Epoch 40/50 Iteration: 91800 Avg. Training loss: 0.5301 0.0067 sec/batch\n",
      "Global Step: 91900 Epoch 40/50 Iteration: 91900 Avg. Training loss: 0.4380 0.0065 sec/batch\n",
      "Global Step: 92000 Epoch 40/50 Iteration: 92000 Avg. Training loss: 0.4895 0.0066 sec/batch\n",
      "Global Step: 92100 Epoch 40/50 Iteration: 92100 Avg. Training loss: 0.4550 0.0069 sec/batch\n",
      "Global Step: 92200 Epoch 40/50 Iteration: 92200 Avg. Training loss: 0.5438 0.0067 sec/batch\n",
      "Global Step: 92300 Epoch 40/50 Iteration: 92300 Avg. Training loss: 0.5244 0.0069 sec/batch\n",
      "Global Step: 92400 Epoch 40/50 Iteration: 92400 Avg. Training loss: 0.5652 0.0071 sec/batch\n",
      "Global Step: 92500 Epoch 40/50 Iteration: 92500 Avg. Training loss: 0.5787 0.0074 sec/batch\n",
      "Global Step: 92600 Epoch 40/50 Iteration: 92600 Avg. Training loss: 0.5441 0.0082 sec/batch\n",
      "Global Step: 92700 Epoch 40/50 Iteration: 92700 Avg. Training loss: 0.5451 0.0065 sec/batch\n",
      "Global Step: 92800 Epoch 40/50 Iteration: 92800 Avg. Training loss: 0.4888 0.0062 sec/batch\n",
      "Global Step: 92900 Epoch 40/50 Iteration: 92900 Avg. Training loss: 0.5236 0.0066 sec/batch\n",
      "Global Step: 93000 Epoch 40/50 Iteration: 93000 Avg. Training loss: 0.4794 0.0064 sec/batch\n",
      "Global Step: 93100 Epoch 40/50 Iteration: 93100 Avg. Training loss: 0.5185 0.0066 sec/batch\n",
      "Global Step: 93200 Epoch 40/50 Iteration: 93200 Avg. Training loss: 0.5139 0.0067 sec/batch\n",
      "Global Step: 93300 Epoch 40/50 Iteration: 93300 Avg. Training loss: 0.4941 0.0062 sec/batch\n",
      "Global Step: 93400 Epoch 40/50 Iteration: 93400 Avg. Training loss: 0.5294 0.0065 sec/batch\n",
      "Global Step: 93500 Epoch 40/50 Iteration: 93500 Avg. Training loss: 0.4462 0.0067 sec/batch\n",
      "Global Step: 93600 Epoch 40/50 Iteration: 93600 Avg. Training loss: 0.5512 0.0066 sec/batch\n",
      "Global Step: 93700 Epoch 40/50 Iteration: 93700 Avg. Training loss: 0.4620 0.0064 sec/batch\n",
      "Global Step: 93800 Epoch 40/50 Iteration: 93800 Avg. Training loss: 0.5189 0.0067 sec/batch\n",
      "Epoch 41/50 Threshold: 0.8296329817038465 Length of Training words: 2482869\n",
      "Global Step: 93900 Epoch 41/50 Iteration: 93900 Avg. Training loss: 0.5800 0.0040 sec/batch\n",
      "Global Step: 94000 Epoch 41/50 Iteration: 94000 Avg. Training loss: 0.5491 0.0063 sec/batch\n",
      "Global Step: 94100 Epoch 41/50 Iteration: 94100 Avg. Training loss: 0.4751 0.0067 sec/batch\n",
      "Global Step: 94200 Epoch 41/50 Iteration: 94200 Avg. Training loss: 0.5904 0.0064 sec/batch\n",
      "Global Step: 94300 Epoch 41/50 Iteration: 94300 Avg. Training loss: 0.4942 0.0065 sec/batch\n",
      "Global Step: 94400 Epoch 41/50 Iteration: 94400 Avg. Training loss: 0.4908 0.0068 sec/batch\n",
      "Global Step: 94500 Epoch 41/50 Iteration: 94500 Avg. Training loss: 0.5013 0.0071 sec/batch\n",
      "Global Step: 94600 Epoch 41/50 Iteration: 94600 Avg. Training loss: 0.4688 0.0087 sec/batch\n",
      "Global Step: 94700 Epoch 41/50 Iteration: 94700 Avg. Training loss: 0.5553 0.0116 sec/batch\n",
      "Global Step: 94800 Epoch 41/50 Iteration: 94800 Avg. Training loss: 0.4887 0.0088 sec/batch\n",
      "Global Step: 94900 Epoch 41/50 Iteration: 94900 Avg. Training loss: 0.5880 0.0106 sec/batch\n",
      "Global Step: 95000 Epoch 41/50 Iteration: 95000 Avg. Training loss: 0.5622 0.0120 sec/batch\n",
      "Global Step: 95100 Epoch 41/50 Iteration: 95100 Avg. Training loss: 0.5766 0.0118 sec/batch\n",
      "Global Step: 95200 Epoch 41/50 Iteration: 95200 Avg. Training loss: 0.5590 0.0118 sec/batch\n",
      "Global Step: 95300 Epoch 41/50 Iteration: 95300 Avg. Training loss: 0.4959 0.0110 sec/batch\n",
      "Global Step: 95400 Epoch 41/50 Iteration: 95400 Avg. Training loss: 0.5250 0.0117 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 95500 Epoch 41/50 Iteration: 95500 Avg. Training loss: 0.4911 0.0136 sec/batch\n",
      "Global Step: 95600 Epoch 41/50 Iteration: 95600 Avg. Training loss: 0.5232 0.0126 sec/batch\n",
      "Global Step: 95700 Epoch 41/50 Iteration: 95700 Avg. Training loss: 0.5195 0.0095 sec/batch\n",
      "Global Step: 95800 Epoch 41/50 Iteration: 95800 Avg. Training loss: 0.4965 0.0104 sec/batch\n",
      "Global Step: 95900 Epoch 41/50 Iteration: 95900 Avg. Training loss: 0.5435 0.0109 sec/batch\n",
      "Global Step: 96000 Epoch 41/50 Iteration: 96000 Avg. Training loss: 0.4714 0.0082 sec/batch\n",
      "Global Step: 96100 Epoch 41/50 Iteration: 96100 Avg. Training loss: 0.5403 0.0099 sec/batch\n",
      "Global Step: 96200 Epoch 41/50 Iteration: 96200 Avg. Training loss: 0.4674 0.0116 sec/batch\n",
      "Global Step: 96300 Epoch 41/50 Iteration: 96300 Avg. Training loss: 0.5456 0.0117 sec/batch\n",
      "Epoch 42/50 Threshold: 0.8199583027465639 Length of Training words: 2469208\n",
      "Global Step: 96400 Epoch 42/50 Iteration: 96400 Avg. Training loss: 0.5746 0.0110 sec/batch\n",
      "Global Step: 96500 Epoch 42/50 Iteration: 96500 Avg. Training loss: 0.5364 0.0133 sec/batch\n",
      "Global Step: 96600 Epoch 42/50 Iteration: 96600 Avg. Training loss: 0.5038 0.0137 sec/batch\n",
      "Global Step: 96700 Epoch 42/50 Iteration: 96700 Avg. Training loss: 0.5681 0.0129 sec/batch\n",
      "Global Step: 96800 Epoch 42/50 Iteration: 96800 Avg. Training loss: 0.5242 0.0132 sec/batch\n",
      "Global Step: 96900 Epoch 42/50 Iteration: 96900 Avg. Training loss: 0.4473 0.0119 sec/batch\n",
      "Global Step: 97000 Epoch 42/50 Iteration: 97000 Avg. Training loss: 0.5001 0.0084 sec/batch\n",
      "Global Step: 97100 Epoch 42/50 Iteration: 97100 Avg. Training loss: 0.4566 0.0070 sec/batch\n",
      "Global Step: 97200 Epoch 42/50 Iteration: 97200 Avg. Training loss: 0.5610 0.0071 sec/batch\n",
      "Global Step: 97300 Epoch 42/50 Iteration: 97300 Avg. Training loss: 0.5409 0.0064 sec/batch\n",
      "Global Step: 97400 Epoch 42/50 Iteration: 97400 Avg. Training loss: 0.5773 0.0069 sec/batch\n",
      "Global Step: 97500 Epoch 42/50 Iteration: 97500 Avg. Training loss: 0.5785 0.0068 sec/batch\n",
      "Global Step: 97600 Epoch 42/50 Iteration: 97600 Avg. Training loss: 0.5461 0.0063 sec/batch\n",
      "Global Step: 97700 Epoch 42/50 Iteration: 97700 Avg. Training loss: 0.5579 0.0064 sec/batch\n",
      "Global Step: 97800 Epoch 42/50 Iteration: 97800 Avg. Training loss: 0.4890 0.0062 sec/batch\n",
      "Global Step: 97900 Epoch 42/50 Iteration: 97900 Avg. Training loss: 0.5200 0.0067 sec/batch\n",
      "Global Step: 98000 Epoch 42/50 Iteration: 98000 Avg. Training loss: 0.5312 0.0063 sec/batch\n",
      "Global Step: 98100 Epoch 42/50 Iteration: 98100 Avg. Training loss: 0.5248 0.0063 sec/batch\n",
      "Global Step: 98200 Epoch 42/50 Iteration: 98200 Avg. Training loss: 0.4791 0.0072 sec/batch\n",
      "Global Step: 98300 Epoch 42/50 Iteration: 98300 Avg. Training loss: 0.5335 0.0081 sec/batch\n",
      "Global Step: 98400 Epoch 42/50 Iteration: 98400 Avg. Training loss: 0.5079 0.0082 sec/batch\n",
      "Global Step: 98500 Epoch 42/50 Iteration: 98500 Avg. Training loss: 0.4668 0.0082 sec/batch\n",
      "Global Step: 98600 Epoch 42/50 Iteration: 98600 Avg. Training loss: 0.5611 0.0093 sec/batch\n",
      "Global Step: 98700 Epoch 42/50 Iteration: 98700 Avg. Training loss: 0.4685 0.0084 sec/batch\n",
      "Epoch 43/50 Threshold: 0.660467435820015 Length of Training words: 2236502\n",
      "Global Step: 98800 Epoch 43/50 Iteration: 98800 Avg. Training loss: 0.5580 0.0004 sec/batch\n",
      "Global Step: 98900 Epoch 43/50 Iteration: 98900 Avg. Training loss: 0.6369 0.0067 sec/batch\n",
      "Global Step: 99000 Epoch 43/50 Iteration: 99000 Avg. Training loss: 0.5226 0.0065 sec/batch\n",
      "Global Step: 99100 Epoch 43/50 Iteration: 99100 Avg. Training loss: 0.6009 0.0067 sec/batch\n",
      "Global Step: 99200 Epoch 43/50 Iteration: 99200 Avg. Training loss: 0.5265 0.0067 sec/batch\n",
      "Global Step: 99300 Epoch 43/50 Iteration: 99300 Avg. Training loss: 0.5240 0.0064 sec/batch\n",
      "Global Step: 99400 Epoch 43/50 Iteration: 99400 Avg. Training loss: 0.5098 0.0066 sec/batch\n",
      "Global Step: 99500 Epoch 43/50 Iteration: 99500 Avg. Training loss: 0.5015 0.0076 sec/batch\n",
      "Global Step: 99600 Epoch 43/50 Iteration: 99600 Avg. Training loss: 0.5766 0.0084 sec/batch\n",
      "Global Step: 99700 Epoch 43/50 Iteration: 99700 Avg. Training loss: 0.6034 0.0070 sec/batch\n",
      "Global Step: 99800 Epoch 43/50 Iteration: 99800 Avg. Training loss: 0.6040 0.0068 sec/batch\n",
      "Global Step: 99900 Epoch 43/50 Iteration: 99900 Avg. Training loss: 0.5883 0.0077 sec/batch\n",
      "Global Step: 100000 Epoch 43/50 Iteration: 100000 Avg. Training loss: 0.5964 0.0083 sec/batch\n",
      "Global Step: 100100 Epoch 43/50 Iteration: 100100 Avg. Training loss: 0.5647 0.0084 sec/batch\n",
      "Global Step: 100200 Epoch 43/50 Iteration: 100200 Avg. Training loss: 0.5521 0.0088 sec/batch\n",
      "Global Step: 100300 Epoch 43/50 Iteration: 100300 Avg. Training loss: 0.5224 0.0083 sec/batch\n",
      "Global Step: 100400 Epoch 43/50 Iteration: 100400 Avg. Training loss: 0.5646 0.0075 sec/batch\n",
      "Global Step: 100500 Epoch 43/50 Iteration: 100500 Avg. Training loss: 0.5048 0.0073 sec/batch\n",
      "Global Step: 100600 Epoch 43/50 Iteration: 100600 Avg. Training loss: 0.5957 0.0069 sec/batch\n",
      "Global Step: 100700 Epoch 43/50 Iteration: 100700 Avg. Training loss: 0.5018 0.0068 sec/batch\n",
      "Global Step: 100800 Epoch 43/50 Iteration: 100800 Avg. Training loss: 0.5515 0.0071 sec/batch\n",
      "Global Step: 100900 Epoch 43/50 Iteration: 100900 Avg. Training loss: 0.5166 0.0073 sec/batch\n",
      "Global Step: 101000 Epoch 43/50 Iteration: 101000 Avg. Training loss: 0.5794 0.0081 sec/batch\n",
      "Epoch 44/50 Threshold: 0.6300008948502464 Length of Training words: 2189679\n",
      "Global Step: 101100 Epoch 44/50 Iteration: 101100 Avg. Training loss: 0.6102 0.0049 sec/batch\n",
      "Global Step: 101200 Epoch 44/50 Iteration: 101200 Avg. Training loss: 0.5697 0.0097 sec/batch\n",
      "Global Step: 101300 Epoch 44/50 Iteration: 101300 Avg. Training loss: 0.5502 0.0075 sec/batch\n",
      "Global Step: 101400 Epoch 44/50 Iteration: 101400 Avg. Training loss: 0.5945 0.0075 sec/batch\n",
      "Global Step: 101500 Epoch 44/50 Iteration: 101500 Avg. Training loss: 0.5379 0.0070 sec/batch\n",
      "Global Step: 101600 Epoch 44/50 Iteration: 101600 Avg. Training loss: 0.5340 0.0070 sec/batch\n",
      "Global Step: 101700 Epoch 44/50 Iteration: 101700 Avg. Training loss: 0.5078 0.0071 sec/batch\n",
      "Global Step: 101800 Epoch 44/50 Iteration: 101800 Avg. Training loss: 0.5671 0.0077 sec/batch\n",
      "Global Step: 101900 Epoch 44/50 Iteration: 101900 Avg. Training loss: 0.5743 0.0071 sec/batch\n",
      "Global Step: 102000 Epoch 44/50 Iteration: 102000 Avg. Training loss: 0.6296 0.0071 sec/batch\n",
      "Global Step: 102100 Epoch 44/50 Iteration: 102100 Avg. Training loss: 0.5992 0.0080 sec/batch\n",
      "Global Step: 102200 Epoch 44/50 Iteration: 102200 Avg. Training loss: 0.5996 0.0092 sec/batch\n",
      "Global Step: 102300 Epoch 44/50 Iteration: 102300 Avg. Training loss: 0.5592 0.0134 sec/batch\n",
      "Global Step: 102400 Epoch 44/50 Iteration: 102400 Avg. Training loss: 0.5530 0.0120 sec/batch\n",
      "Global Step: 102500 Epoch 44/50 Iteration: 102500 Avg. Training loss: 0.5427 0.0126 sec/batch\n",
      "Global Step: 102600 Epoch 44/50 Iteration: 102600 Avg. Training loss: 0.5693 0.0093 sec/batch\n",
      "Global Step: 102700 Epoch 44/50 Iteration: 102700 Avg. Training loss: 0.5152 0.0085 sec/batch\n",
      "Global Step: 102800 Epoch 44/50 Iteration: 102800 Avg. Training loss: 0.6025 0.0075 sec/batch\n",
      "Global Step: 102900 Epoch 44/50 Iteration: 102900 Avg. Training loss: 0.5124 0.0073 sec/batch\n",
      "Global Step: 103000 Epoch 44/50 Iteration: 103000 Avg. Training loss: 0.5634 0.0072 sec/batch\n",
      "Global Step: 103100 Epoch 44/50 Iteration: 103100 Avg. Training loss: 0.5216 0.0079 sec/batch\n",
      "Global Step: 103200 Epoch 44/50 Iteration: 103200 Avg. Training loss: 0.5787 0.0072 sec/batch\n",
      "Epoch 45/50 Threshold: 0.609421549769984 Length of Training words: 2155484\n",
      "Global Step: 103300 Epoch 45/50 Iteration: 103300 Avg. Training loss: 0.6333 0.0068 sec/batch\n",
      "Global Step: 103400 Epoch 45/50 Iteration: 103400 Avg. Training loss: 0.5471 0.0127 sec/batch\n",
      "Global Step: 103500 Epoch 45/50 Iteration: 103500 Avg. Training loss: 0.5802 0.0121 sec/batch\n",
      "Global Step: 103600 Epoch 45/50 Iteration: 103600 Avg. Training loss: 0.5848 0.0123 sec/batch\n",
      "Global Step: 103700 Epoch 45/50 Iteration: 103700 Avg. Training loss: 0.5142 0.0116 sec/batch\n",
      "Global Step: 103800 Epoch 45/50 Iteration: 103800 Avg. Training loss: 0.5288 0.0083 sec/batch\n",
      "Global Step: 103900 Epoch 45/50 Iteration: 103900 Avg. Training loss: 0.5089 0.0083 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 104000 Epoch 45/50 Iteration: 104000 Avg. Training loss: 0.5951 0.0076 sec/batch\n",
      "Global Step: 104100 Epoch 45/50 Iteration: 104100 Avg. Training loss: 0.6184 0.0075 sec/batch\n",
      "Global Step: 104200 Epoch 45/50 Iteration: 104200 Avg. Training loss: 0.5952 0.0072 sec/batch\n",
      "Global Step: 104300 Epoch 45/50 Iteration: 104300 Avg. Training loss: 0.6169 0.0074 sec/batch\n",
      "Global Step: 104400 Epoch 45/50 Iteration: 104400 Avg. Training loss: 0.6125 0.0085 sec/batch\n",
      "Global Step: 104500 Epoch 45/50 Iteration: 104500 Avg. Training loss: 0.5267 0.0096 sec/batch\n",
      "Global Step: 104600 Epoch 45/50 Iteration: 104600 Avg. Training loss: 0.5579 0.0093 sec/batch\n",
      "Global Step: 104700 Epoch 45/50 Iteration: 104700 Avg. Training loss: 0.6163 0.0106 sec/batch\n",
      "Global Step: 104800 Epoch 45/50 Iteration: 104800 Avg. Training loss: 0.4844 0.0107 sec/batch\n",
      "Global Step: 104900 Epoch 45/50 Iteration: 104900 Avg. Training loss: 0.5458 0.0114 sec/batch\n",
      "Global Step: 105000 Epoch 45/50 Iteration: 105000 Avg. Training loss: 0.6094 0.0092 sec/batch\n",
      "Global Step: 105100 Epoch 45/50 Iteration: 105100 Avg. Training loss: 0.5015 0.0087 sec/batch\n",
      "Global Step: 105200 Epoch 45/50 Iteration: 105200 Avg. Training loss: 0.5841 0.0086 sec/batch\n",
      "Global Step: 105300 Epoch 45/50 Iteration: 105300 Avg. Training loss: 0.5228 0.0090 sec/batch\n",
      "Epoch 46/50 Threshold: 0.6445462199353563 Length of Training words: 2212658\n",
      "Global Step: 105400 Epoch 46/50 Iteration: 105400 Avg. Training loss: 0.6109 0.0019 sec/batch\n",
      "Global Step: 105500 Epoch 46/50 Iteration: 105500 Avg. Training loss: 0.6173 0.0087 sec/batch\n",
      "Global Step: 105600 Epoch 46/50 Iteration: 105600 Avg. Training loss: 0.5077 0.0111 sec/batch\n",
      "Global Step: 105700 Epoch 46/50 Iteration: 105700 Avg. Training loss: 0.6030 0.0102 sec/batch\n",
      "Global Step: 105800 Epoch 46/50 Iteration: 105800 Avg. Training loss: 0.5698 0.0087 sec/batch\n",
      "Global Step: 105900 Epoch 46/50 Iteration: 105900 Avg. Training loss: 0.4922 0.0090 sec/batch\n",
      "Global Step: 106000 Epoch 46/50 Iteration: 106000 Avg. Training loss: 0.5345 0.0092 sec/batch\n",
      "Global Step: 106100 Epoch 46/50 Iteration: 106100 Avg. Training loss: 0.5480 0.0096 sec/batch\n",
      "Global Step: 106200 Epoch 46/50 Iteration: 106200 Avg. Training loss: 0.5399 0.0094 sec/batch\n",
      "Global Step: 106300 Epoch 46/50 Iteration: 106300 Avg. Training loss: 0.6276 0.0087 sec/batch\n",
      "Global Step: 106400 Epoch 46/50 Iteration: 106400 Avg. Training loss: 0.5727 0.0084 sec/batch\n",
      "Global Step: 106500 Epoch 46/50 Iteration: 106500 Avg. Training loss: 0.6229 0.0083 sec/batch\n",
      "Global Step: 106600 Epoch 46/50 Iteration: 106600 Avg. Training loss: 0.5829 0.0087 sec/batch\n",
      "Global Step: 106700 Epoch 46/50 Iteration: 106700 Avg. Training loss: 0.5186 0.0087 sec/batch\n",
      "Global Step: 106800 Epoch 46/50 Iteration: 106800 Avg. Training loss: 0.5588 0.0087 sec/batch\n",
      "Global Step: 106900 Epoch 46/50 Iteration: 106900 Avg. Training loss: 0.6119 0.0086 sec/batch\n",
      "Global Step: 107000 Epoch 46/50 Iteration: 107000 Avg. Training loss: 0.4751 0.0087 sec/batch\n",
      "Global Step: 107100 Epoch 46/50 Iteration: 107100 Avg. Training loss: 0.5339 0.0083 sec/batch\n",
      "Global Step: 107200 Epoch 46/50 Iteration: 107200 Avg. Training loss: 0.6013 0.0086 sec/batch\n",
      "Global Step: 107300 Epoch 46/50 Iteration: 107300 Avg. Training loss: 0.4975 0.0087 sec/batch\n",
      "Global Step: 107400 Epoch 46/50 Iteration: 107400 Avg. Training loss: 0.5754 0.0085 sec/batch\n",
      "Global Step: 107500 Epoch 46/50 Iteration: 107500 Avg. Training loss: 0.5057 0.0086 sec/batch\n",
      "Epoch 47/50 Threshold: 0.7736997914410632 Length of Training words: 2403854\n",
      "Global Step: 107600 Epoch 47/50 Iteration: 107600 Avg. Training loss: 0.5914 0.0011 sec/batch\n",
      "Global Step: 107700 Epoch 47/50 Iteration: 107700 Avg. Training loss: 0.6072 0.0088 sec/batch\n",
      "Global Step: 107800 Epoch 47/50 Iteration: 107800 Avg. Training loss: 0.4977 0.0086 sec/batch\n",
      "Global Step: 107900 Epoch 47/50 Iteration: 107900 Avg. Training loss: 0.5549 0.0085 sec/batch\n",
      "Global Step: 108000 Epoch 47/50 Iteration: 108000 Avg. Training loss: 0.5514 0.0084 sec/batch\n",
      "Global Step: 108100 Epoch 47/50 Iteration: 108100 Avg. Training loss: 0.4999 0.0081 sec/batch\n",
      "Global Step: 108200 Epoch 47/50 Iteration: 108200 Avg. Training loss: 0.4998 0.0084 sec/batch\n",
      "Global Step: 108300 Epoch 47/50 Iteration: 108300 Avg. Training loss: 0.4975 0.0084 sec/batch\n",
      "Global Step: 108400 Epoch 47/50 Iteration: 108400 Avg. Training loss: 0.5241 0.0085 sec/batch\n",
      "Global Step: 108500 Epoch 47/50 Iteration: 108500 Avg. Training loss: 0.4998 0.0084 sec/batch\n",
      "Global Step: 108600 Epoch 47/50 Iteration: 108600 Avg. Training loss: 0.6118 0.0086 sec/batch\n",
      "Global Step: 108700 Epoch 47/50 Iteration: 108700 Avg. Training loss: 0.5542 0.0084 sec/batch\n",
      "Global Step: 108800 Epoch 47/50 Iteration: 108800 Avg. Training loss: 0.5931 0.0083 sec/batch\n",
      "Global Step: 108900 Epoch 47/50 Iteration: 108900 Avg. Training loss: 0.5709 0.0088 sec/batch\n",
      "Global Step: 109000 Epoch 47/50 Iteration: 109000 Avg. Training loss: 0.5013 0.0092 sec/batch\n",
      "Global Step: 109100 Epoch 47/50 Iteration: 109100 Avg. Training loss: 0.5392 0.0082 sec/batch\n",
      "Global Step: 109200 Epoch 47/50 Iteration: 109200 Avg. Training loss: 0.5045 0.0087 sec/batch\n",
      "Global Step: 109300 Epoch 47/50 Iteration: 109300 Avg. Training loss: 0.5393 0.0082 sec/batch\n",
      "Global Step: 109400 Epoch 47/50 Iteration: 109400 Avg. Training loss: 0.4913 0.0092 sec/batch\n",
      "Global Step: 109500 Epoch 47/50 Iteration: 109500 Avg. Training loss: 0.5255 0.0087 sec/batch\n",
      "Global Step: 109600 Epoch 47/50 Iteration: 109600 Avg. Training loss: 0.5387 0.0104 sec/batch\n",
      "Global Step: 109700 Epoch 47/50 Iteration: 109700 Avg. Training loss: 0.4782 0.0116 sec/batch\n",
      "Global Step: 109800 Epoch 47/50 Iteration: 109800 Avg. Training loss: 0.5619 0.0091 sec/batch\n",
      "Global Step: 109900 Epoch 47/50 Iteration: 109900 Avg. Training loss: 0.4766 0.0117 sec/batch\n",
      "Epoch 48/50 Threshold: 0.6727853940847345 Length of Training words: 2256697\n",
      "Global Step: 110000 Epoch 48/50 Iteration: 110000 Avg. Training loss: 0.5703 0.0010 sec/batch\n",
      "Global Step: 110100 Epoch 48/50 Iteration: 110100 Avg. Training loss: 0.6205 0.0101 sec/batch\n",
      "Global Step: 110200 Epoch 48/50 Iteration: 110200 Avg. Training loss: 0.5320 0.0082 sec/batch\n",
      "Global Step: 110300 Epoch 48/50 Iteration: 110300 Avg. Training loss: 0.5948 0.0084 sec/batch\n",
      "Global Step: 110400 Epoch 48/50 Iteration: 110400 Avg. Training loss: 0.5221 0.0079 sec/batch\n",
      "Global Step: 110500 Epoch 48/50 Iteration: 110500 Avg. Training loss: 0.5219 0.0099 sec/batch\n",
      "Global Step: 110600 Epoch 48/50 Iteration: 110600 Avg. Training loss: 0.5086 0.0090 sec/batch\n",
      "Global Step: 110700 Epoch 48/50 Iteration: 110700 Avg. Training loss: 0.4968 0.0088 sec/batch\n",
      "Global Step: 110800 Epoch 48/50 Iteration: 110800 Avg. Training loss: 0.5743 0.0084 sec/batch\n",
      "Global Step: 110900 Epoch 48/50 Iteration: 110900 Avg. Training loss: 0.6010 0.0081 sec/batch\n",
      "Global Step: 111000 Epoch 48/50 Iteration: 111000 Avg. Training loss: 0.6069 0.0079 sec/batch\n",
      "Global Step: 111100 Epoch 48/50 Iteration: 111100 Avg. Training loss: 0.5884 0.0081 sec/batch\n",
      "Global Step: 111200 Epoch 48/50 Iteration: 111200 Avg. Training loss: 0.5877 0.0073 sec/batch\n",
      "Global Step: 111300 Epoch 48/50 Iteration: 111300 Avg. Training loss: 0.5473 0.0084 sec/batch\n",
      "Global Step: 111400 Epoch 48/50 Iteration: 111400 Avg. Training loss: 0.5430 0.0090 sec/batch\n",
      "Global Step: 111500 Epoch 48/50 Iteration: 111500 Avg. Training loss: 0.5309 0.0077 sec/batch\n",
      "Global Step: 111600 Epoch 48/50 Iteration: 111600 Avg. Training loss: 0.5698 0.0080 sec/batch\n",
      "Global Step: 111700 Epoch 48/50 Iteration: 111700 Avg. Training loss: 0.5047 0.0084 sec/batch\n",
      "Global Step: 111800 Epoch 48/50 Iteration: 111800 Avg. Training loss: 0.5741 0.0099 sec/batch\n",
      "Global Step: 111900 Epoch 48/50 Iteration: 111900 Avg. Training loss: 0.5314 0.0090 sec/batch\n",
      "Global Step: 112000 Epoch 48/50 Iteration: 112000 Avg. Training loss: 0.5069 0.0092 sec/batch\n",
      "Global Step: 112100 Epoch 48/50 Iteration: 112100 Avg. Training loss: 0.5802 0.0084 sec/batch\n",
      "Global Step: 112200 Epoch 48/50 Iteration: 112200 Avg. Training loss: 0.5039 0.0084 sec/batch\n",
      "Epoch 49/50 Threshold: 0.8258254025302388 Length of Training words: 2477639\n",
      "Global Step: 112300 Epoch 49/50 Iteration: 112300 Avg. Training loss: 0.6090 0.0048 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Step: 112400 Epoch 49/50 Iteration: 112400 Avg. Training loss: 0.5557 0.0091 sec/batch\n",
      "Global Step: 112500 Epoch 49/50 Iteration: 112500 Avg. Training loss: 0.4777 0.0084 sec/batch\n",
      "Global Step: 112600 Epoch 49/50 Iteration: 112600 Avg. Training loss: 0.5912 0.0088 sec/batch\n",
      "Global Step: 112700 Epoch 49/50 Iteration: 112700 Avg. Training loss: 0.4891 0.0097 sec/batch\n",
      "Global Step: 112800 Epoch 49/50 Iteration: 112800 Avg. Training loss: 0.4956 0.0097 sec/batch\n",
      "Global Step: 112900 Epoch 49/50 Iteration: 112900 Avg. Training loss: 0.5006 0.0101 sec/batch\n",
      "Global Step: 113000 Epoch 49/50 Iteration: 113000 Avg. Training loss: 0.4658 0.0094 sec/batch\n",
      "Global Step: 113100 Epoch 49/50 Iteration: 113100 Avg. Training loss: 0.5550 0.0083 sec/batch\n",
      "Global Step: 113200 Epoch 49/50 Iteration: 113200 Avg. Training loss: 0.4864 0.0084 sec/batch\n",
      "Global Step: 113300 Epoch 49/50 Iteration: 113300 Avg. Training loss: 0.5869 0.0081 sec/batch\n",
      "Global Step: 113400 Epoch 49/50 Iteration: 113400 Avg. Training loss: 0.5626 0.0085 sec/batch\n",
      "Global Step: 113500 Epoch 49/50 Iteration: 113500 Avg. Training loss: 0.5798 0.0085 sec/batch\n",
      "Global Step: 113600 Epoch 49/50 Iteration: 113600 Avg. Training loss: 0.5632 0.0084 sec/batch\n",
      "Global Step: 113700 Epoch 49/50 Iteration: 113700 Avg. Training loss: 0.4945 0.0075 sec/batch\n",
      "Global Step: 113800 Epoch 49/50 Iteration: 113800 Avg. Training loss: 0.5304 0.0070 sec/batch\n",
      "Global Step: 113900 Epoch 49/50 Iteration: 113900 Avg. Training loss: 0.4890 0.0078 sec/batch\n",
      "Global Step: 114000 Epoch 49/50 Iteration: 114000 Avg. Training loss: 0.5219 0.0077 sec/batch\n",
      "Global Step: 114100 Epoch 49/50 Iteration: 114100 Avg. Training loss: 0.5194 0.0076 sec/batch\n",
      "Global Step: 114200 Epoch 49/50 Iteration: 114200 Avg. Training loss: 0.4990 0.0075 sec/batch\n",
      "Global Step: 114300 Epoch 49/50 Iteration: 114300 Avg. Training loss: 0.5458 0.0072 sec/batch\n",
      "Global Step: 114400 Epoch 49/50 Iteration: 114400 Avg. Training loss: 0.4630 0.0079 sec/batch\n",
      "Global Step: 114500 Epoch 49/50 Iteration: 114500 Avg. Training loss: 0.5519 0.0075 sec/batch\n",
      "Global Step: 114600 Epoch 49/50 Iteration: 114600 Avg. Training loss: 0.4690 0.0071 sec/batch\n",
      "Global Step: 114700 Epoch 49/50 Iteration: 114700 Avg. Training loss: 0.5472 0.0073 sec/batch\n",
      "Epoch 50/50 Threshold: 0.603864652909913 Length of Training words: 2148543\n",
      "Global Step: 114800 Epoch 50/50 Iteration: 114800 Avg. Training loss: 0.6174 0.0057 sec/batch\n",
      "Global Step: 114900 Epoch 50/50 Iteration: 114900 Avg. Training loss: 0.5552 0.0073 sec/batch\n",
      "Global Step: 115000 Epoch 50/50 Iteration: 115000 Avg. Training loss: 0.5840 0.0075 sec/batch\n",
      "Global Step: 115100 Epoch 50/50 Iteration: 115100 Avg. Training loss: 0.5898 0.0074 sec/batch\n",
      "Global Step: 115200 Epoch 50/50 Iteration: 115200 Avg. Training loss: 0.5164 0.0086 sec/batch\n",
      "Global Step: 115300 Epoch 50/50 Iteration: 115300 Avg. Training loss: 0.5278 0.0079 sec/batch\n",
      "Global Step: 115400 Epoch 50/50 Iteration: 115400 Avg. Training loss: 0.5082 0.0077 sec/batch\n",
      "Global Step: 115500 Epoch 50/50 Iteration: 115500 Avg. Training loss: 0.5950 0.0073 sec/batch\n",
      "Global Step: 115600 Epoch 50/50 Iteration: 115600 Avg. Training loss: 0.6220 0.0071 sec/batch\n",
      "Global Step: 115700 Epoch 50/50 Iteration: 115700 Avg. Training loss: 0.5965 0.0072 sec/batch\n",
      "Global Step: 115800 Epoch 50/50 Iteration: 115800 Avg. Training loss: 0.6195 0.0072 sec/batch\n",
      "Global Step: 115900 Epoch 50/50 Iteration: 115900 Avg. Training loss: 0.6120 0.0073 sec/batch\n",
      "Global Step: 116000 Epoch 50/50 Iteration: 116000 Avg. Training loss: 0.5219 0.0072 sec/batch\n",
      "Global Step: 116100 Epoch 50/50 Iteration: 116100 Avg. Training loss: 0.5678 0.0073 sec/batch\n",
      "Global Step: 116200 Epoch 50/50 Iteration: 116200 Avg. Training loss: 0.6172 0.0074 sec/batch\n",
      "Global Step: 116300 Epoch 50/50 Iteration: 116300 Avg. Training loss: 0.4856 0.0070 sec/batch\n",
      "Global Step: 116400 Epoch 50/50 Iteration: 116400 Avg. Training loss: 0.5485 0.0088 sec/batch\n",
      "Global Step: 116500 Epoch 50/50 Iteration: 116500 Avg. Training loss: 0.6074 0.0076 sec/batch\n",
      "Global Step: 116600 Epoch 50/50 Iteration: 116600 Avg. Training loss: 0.5035 0.0075 sec/batch\n",
      "Global Step: 116700 Epoch 50/50 Iteration: 116700 Avg. Training loss: 0.5864 0.0071 sec/batch\n",
      "Global Step: 116800 Epoch 50/50 Iteration: 116800 Avg. Training loss: 0.5191 0.0073 sec/batch\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "#     saver.restore(sess, tf.train.latest_checkpoint('checkpoints/ner'))\n",
    "#     embed_mat = sess.run(embedding)\n",
    "    \n",
    "    for e in range(1, epochs+1):\n",
    "        train_words, threshold = get_train_word()\n",
    "        print(\"Epoch {}/{}\".format(e, epochs), \"Threshold: {}\".format(threshold), \"Length of Training words: {}\".format(len(train_words)))\n",
    "        batches = get_batches(train_words, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            global_steps, train_loss, _ = sess.run([global_step, cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100== 0: \n",
    "                end = time.time()\n",
    "                print(\"Global Step: {}\".format(global_steps), \"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "                        \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/ner/ner.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/ner/ner.ckpt\n"
     ]
    }
   ],
   "source": [
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('checkpoints/ner'))\n",
    "    embed_mat = sess.run(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAvoAAALNCAYAAABeYMXAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS5IkSRn0/wGMZBOFCbVOCgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb2a7185c50>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 358,
       "width": 381
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "viz_words = n_vocab\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_words, :])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for idx in range(viz_words):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='blue')\n",
    "    plt.annotate(int_to_vocab[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=1, xytext=(embed_tsne[idx, 0]+1.5, embed_tsne[idx, 1]+1.5), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
